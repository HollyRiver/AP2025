{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1757985678650,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "dlNZ7OmPpD-4"
   },
   "outputs": [],
   "source": [
    "#  https://tutorials.pytorch.kr/beginner/knowledge_distillation_tutorial.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1532,
     "status": "ok",
     "timestamp": 1757985681216,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "toet7KWipGct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:16<00:00, 10.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    ## 표준화. 채널별\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757985681220,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "I9LCivo4pkve"
   },
   "outputs": [],
   "source": [
    "#Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757984834635,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "Ow5EBBMgpUm4"
   },
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)    ## Convolution Feature Extracting\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1757984970625,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "nUr-4fDYpVoi"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    각 아키텍쳐 별 따로 훈련\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490545,
     "status": "ok",
     "timestamp": 1757986187694,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "XV6LNjTJpZgI",
    "outputId": "87131bbc-00c2-45a1-cd86-ce84ddfe6ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.336433367351132\n",
      "Epoch 2/20, Loss: 0.8625639736499933\n",
      "Epoch 3/20, Loss: 0.6757945669886402\n",
      "Epoch 4/20, Loss: 0.5359418667338388\n",
      "Epoch 5/20, Loss: 0.41381668114601194\n",
      "Epoch 6/20, Loss: 0.3020941295739635\n",
      "Epoch 7/20, Loss: 0.21628727053132507\n",
      "Epoch 8/20, Loss: 0.17484693114867295\n",
      "Epoch 9/20, Loss: 0.1456941222328969\n",
      "Epoch 10/20, Loss: 0.11994618994881735\n",
      "Epoch 11/20, Loss: 0.10613784218287986\n",
      "Epoch 12/20, Loss: 0.0936117332304835\n",
      "Epoch 13/20, Loss: 0.08769675203220313\n",
      "Epoch 14/20, Loss: 0.08408447083733651\n",
      "Epoch 15/20, Loss: 0.07831507551309931\n",
      "Epoch 16/20, Loss: 0.07299153961698569\n",
      "Epoch 17/20, Loss: 0.08274304628124475\n",
      "Epoch 18/20, Loss: 0.06859014522108009\n",
      "Epoch 19/20, Loss: 0.07130030811885776\n",
      "Epoch 20/20, Loss: 0.06357712858854353\n",
      "Test Accuracy: 75.51%\n",
      "Epoch 1/20, Loss: 1.469579372259662\n",
      "Epoch 2/20, Loss: 1.15665774272226\n",
      "Epoch 3/20, Loss: 1.0241795825531415\n",
      "Epoch 4/20, Loss: 0.9234135222556951\n",
      "Epoch 5/20, Loss: 0.8502556491081062\n",
      "Epoch 6/20, Loss: 0.783008501505303\n",
      "Epoch 7/20, Loss: 0.7165057451828666\n",
      "Epoch 8/20, Loss: 0.6579855926658796\n",
      "Epoch 9/20, Loss: 0.6072278166061167\n",
      "Epoch 10/20, Loss: 0.5544767587843453\n",
      "Epoch 11/20, Loss: 0.5109488625660576\n",
      "Epoch 12/20, Loss: 0.46103175079731074\n",
      "Epoch 13/20, Loss: 0.4215944344582765\n",
      "Epoch 14/20, Loss: 0.38222321048569496\n",
      "Epoch 15/20, Loss: 0.34851220943738737\n",
      "Epoch 16/20, Loss: 0.3163056772230836\n",
      "Epoch 17/20, Loss: 0.2869813930424278\n",
      "Epoch 18/20, Loss: 0.25940939054236084\n",
      "Epoch 19/20, Loss: 0.24171306230985296\n",
      "Epoch 20/20, Loss: 0.2194795693697222\n",
      "Test Accuracy: 68.74%\n",
      "Teacher accuracy: 75.51%\n",
      "Student accuracy: 68.74%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=20, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)\n",
    "\n",
    "train(nn_light, train_loader, epochs=20, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)\n",
    "\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757986208339,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "Z8asKODhsKVY",
    "outputId": "7d05376b-1a6c-41b6-fb1f-a48712c2f8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,186,986\n",
      "LightNN parameters: 267,738\n"
     ]
    }
   ],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252044,
     "status": "ok",
     "timestamp": 1757986500757,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "Nec7zeu_qPOm",
    "outputId": "a7deaf87-98e9-4684-db46-066ecb0656f7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m train_knowledge_distillation(teacher=\u001b[43mnn_deep\u001b[49m, student=new_nn_light, train_loader=train_loader, epochs=\u001b[32m20\u001b[39m, learning_rate=\u001b[32m0.001\u001b[39m, T=\u001b[32m2\u001b[39m, \u001b[38;5;66;03m## softmax의 sharpness를 조정 (1보다 크므로 클래스별 확률 차이를 완화)\u001b[39;00m\n\u001b[32m     44\u001b[39m                               soft_target_loss_weight=\u001b[32m0.25\u001b[39m, ce_loss_weight=\u001b[32m0.75\u001b[39m, device=device)\n\u001b[32m     45\u001b[39m test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Compare the student test accuracy with and without the teacher, after distillation\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'nn_deep' is not defined"
     ]
    }
   ],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    \"\"\"\n",
    "    Teacher는 건드리지 않고, Student만 학습. 증류\n",
    "    \"\"\"\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)    ## Temperature Scaling\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)   ## 합연산으로 전환, 연산 시 수치적 안정성 확보\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)   ## KL divergence 계산, Response-Base\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss    ## 손실의 가중치 설정. 일반적으로 Distillation Loss를 작게 설정하는듯\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=20, learning_rate=0.001, T=2, ## softmax의 sharpness를 조정 (1보다 크므로 클래스별 확률 차이를 완화)\n",
    "                              soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어느정도 성능 향상이 있긴 하나, 그렇게 큰 차이는 나지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1757986523391,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "wYny3pLgqDEQ",
    "outputId": "63f033fd-4986-4efa-d6c5-3c2222b2c05b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer for deep_nn: 7.753559589385986\n",
      "Norm of 1st layer for modified_deep_nn: 7.753559589385986\n",
      "Norm of 1st layer: 2.327361822128296\n",
      "Norm of 1st layer: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "class ModifiedDeepNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
    "        return x, flattened_conv_output_after_pooling\n",
    "\n",
    "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
    "class ModifiedLightNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        return x, flattened_conv_output\n",
    "\n",
    "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
    "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
    "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "# Once again ensure the norm of the first layer is the same for both networks\n",
    "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
    "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
    "\n",
    "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
    "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())\n",
    "\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
    "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757986528095,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "iRxcXPLADfMu",
    "outputId": "23aace36-2dd9-4281-d96f-881398a5a6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student logits shape: torch.Size([128, 10])\n",
      "Student hidden representation shape: torch.Size([128, 1024])\n",
      "Teacher logits shape: torch.Size([128, 10])\n",
      "Teacher hidden representation shape: torch.Size([128, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor\n",
    "sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n",
    "\n",
    "# Pass the input through the student\n",
    "logits, hidden_representation = modified_nn_light(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
    "\n",
    "# Pass the input through the teacher\n",
    "logits, hidden_representation = modified_nn_deep(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757985481374,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "Ml0dijRxDj96"
   },
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()      ## hidden layer vector간 Cosine Loss를 사용\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model and keep only the hidden representation\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "\n",
    "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))   ## cosine_loss 활용, 성능 개선됨\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757986543042,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "rnKpNFTBDqQL"
   },
   "outputs": [],
   "source": [
    "def test_multiple_outputs(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258976,
     "status": "ok",
     "timestamp": 1757986809690,
     "user": {
      "displayName": "김광수",
      "userId": "04260902725982947063"
     },
     "user_tz": -540
    },
    "id": "N4xF4owaGf8M",
    "outputId": "a849f2ed-bf22-43bb-fa64-8ff5236d711e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.308491258669997\n",
      "Epoch 2/20, Loss: 1.0721526258741803\n",
      "Epoch 3/20, Loss: 0.9723540135966543\n",
      "Epoch 4/20, Loss: 0.8976628324564766\n",
      "Epoch 5/20, Loss: 0.8460214583160323\n",
      "Epoch 6/20, Loss: 0.8008004228782166\n",
      "Epoch 7/20, Loss: 0.7577904174699808\n",
      "Epoch 8/20, Loss: 0.7235273083152673\n",
      "Epoch 9/20, Loss: 0.6873242481590232\n",
      "Epoch 10/20, Loss: 0.6602568071516578\n",
      "Epoch 11/20, Loss: 0.6303940819352484\n",
      "Epoch 12/20, Loss: 0.6021087020254501\n",
      "Epoch 13/20, Loss: 0.5730948635684255\n",
      "Epoch 14/20, Loss: 0.5505662844766437\n",
      "Epoch 15/20, Loss: 0.5196736778139763\n",
      "Epoch 16/20, Loss: 0.4983766588865948\n",
      "Epoch 17/20, Loss: 0.47957804074982546\n",
      "Epoch 18/20, Loss: 0.46000341869071315\n",
      "Epoch 19/20, Loss: 0.4416675258932821\n",
      "Epoch 20/20, Loss: 0.4236981900756621\n",
      "Test Accuracy: 71.17%\n"
     ]
    }
   ],
   "source": [
    "# Train and test the lightweight network with cross entropy loss\n",
    "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=20, learning_rate=0.001,\n",
    "                  hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMidddAFaD2kkSSbI0MGUgw",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
