{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMidddAFaD2kkSSbI0MGUgw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":17,"metadata":{"id":"dlNZ7OmPpD-4","executionInfo":{"status":"ok","timestamp":1757985678650,"user_tz":-540,"elapsed":43,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"outputs":[],"source":["#  https://tutorials.pytorch.kr/beginner/knowledge_distillation_tutorial.html\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","# Check if GPU is available, and if not, use the CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n","transforms_cifar = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Loading the CIFAR-10 dataset:\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n"],"metadata":{"id":"toet7KWipGct","executionInfo":{"status":"ok","timestamp":1757985681216,"user_tz":-540,"elapsed":1532,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#Dataloaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"],"metadata":{"id":"I9LCivo4pkve","executionInfo":{"status":"ok","timestamp":1757985681220,"user_tz":-540,"elapsed":1,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Deeper neural network class to be used as teacher:\n","class DeepNN(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(DeepNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","# Lightweight neural network class to be used as student:\n","class LightNN(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(LightNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(1024, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"Ow5EBBMgpUm4","executionInfo":{"status":"ok","timestamp":1757984834635,"user_tz":-540,"elapsed":3,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, epochs, learning_rate, device):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            # inputs: A collection of batch_size images\n","            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","\n","            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n","            # labels: The actual labels of the images. Vector of dimensionality batch_size\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n","\n","def test(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Test Accuracy: {accuracy:.2f}%\")\n","    return accuracy"],"metadata":{"id":"nUr-4fDYpVoi","executionInfo":{"status":"ok","timestamp":1757984970625,"user_tz":-540,"elapsed":43,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","nn_deep = DeepNN(num_classes=10).to(device)\n","train(nn_deep, train_loader, epochs=20, learning_rate=0.001, device=device)\n","test_accuracy_deep = test(nn_deep, test_loader, device)\n","\n","# Instantiate the lightweight network:\n","torch.manual_seed(42)\n","nn_light = LightNN(num_classes=10).to(device)\n","\n","train(nn_light, train_loader, epochs=20, learning_rate=0.001, device=device)\n","test_accuracy_light_ce = test(nn_light, test_loader, device)\n","\n","print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n","print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XV6LNjTJpZgI","outputId":"87131bbc-00c2-45a1-cd86-ce84ddfe6ab5","executionInfo":{"status":"ok","timestamp":1757986187694,"user_tz":-540,"elapsed":490545,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 1.330593869966619\n","Epoch 2/20, Loss: 0.8697309151025074\n","Epoch 3/20, Loss: 0.6808838004346394\n","Epoch 4/20, Loss: 0.5400588694588303\n","Epoch 5/20, Loss: 0.4243079987557038\n","Epoch 6/20, Loss: 0.3233232378502331\n","Epoch 7/20, Loss: 0.23608669880634683\n","Epoch 8/20, Loss: 0.1853787641200568\n","Epoch 9/20, Loss: 0.14829771905718253\n","Epoch 10/20, Loss: 0.1224339989673756\n","Epoch 11/20, Loss: 0.1118378219864024\n","Epoch 12/20, Loss: 0.09751334769741805\n","Epoch 13/20, Loss: 0.09536976082360044\n","Epoch 14/20, Loss: 0.08084478514397617\n","Epoch 15/20, Loss: 0.0861307409117975\n","Epoch 16/20, Loss: 0.08396665518269743\n","Epoch 17/20, Loss: 0.08453464818418102\n","Epoch 18/20, Loss: 0.07384500050883921\n","Epoch 19/20, Loss: 0.06506284302257745\n","Epoch 20/20, Loss: 0.06414430265140998\n","Test Accuracy: 75.57%\n","Epoch 1/20, Loss: 1.469978882528632\n","Epoch 2/20, Loss: 1.1616020920636403\n","Epoch 3/20, Loss: 1.0334521361324183\n","Epoch 4/20, Loss: 0.9331328849048566\n","Epoch 5/20, Loss: 0.8586609487033561\n","Epoch 6/20, Loss: 0.7921414940863314\n","Epoch 7/20, Loss: 0.7259081359714499\n","Epoch 8/20, Loss: 0.6658034028909395\n","Epoch 9/20, Loss: 0.6130068801400607\n","Epoch 10/20, Loss: 0.5609065997021278\n","Epoch 11/20, Loss: 0.5131913167436409\n","Epoch 12/20, Loss: 0.46779585631607135\n","Epoch 13/20, Loss: 0.4262720228308607\n","Epoch 14/20, Loss: 0.38669087068961405\n","Epoch 15/20, Loss: 0.35425930757961616\n","Epoch 16/20, Loss: 0.3169816775471353\n","Epoch 17/20, Loss: 0.2886920941379064\n","Epoch 18/20, Loss: 0.269114398780991\n","Epoch 19/20, Loss: 0.2452089825973791\n","Epoch 20/20, Loss: 0.2266824631511098\n","Test Accuracy: 69.11%\n","Teacher accuracy: 75.57%\n","Student accuracy: 69.11%\n"]}]},{"cell_type":"code","source":["total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n","print(f\"DeepNN parameters: {total_params_deep}\")\n","total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n","print(f\"LightNN parameters: {total_params_light}\")\n","\n","torch.manual_seed(42)\n","new_nn_light = LightNN(num_classes=10).to(device)"],"metadata":{"id":"Z8asKODhsKVY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757986208339,"user_tz":-540,"elapsed":14,"user":{"displayName":"김광수","userId":"04260902725982947063"}},"outputId":"7d05376b-1a6c-41b6-fb1f-a48712c2f8ac"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["DeepNN parameters: 1,186,986\n","LightNN parameters: 267,738\n"]}]},{"cell_type":"code","source":["def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n","    ce_loss = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n","\n","    teacher.eval()  # Teacher set to evaluation mode\n","    student.train() # Student to train mode\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n","            with torch.no_grad():\n","                teacher_logits = teacher(inputs)\n","\n","            # Forward pass with the student model\n","            student_logits = student(inputs)\n","\n","            #Soften the student logits by applying softmax first and log() second\n","            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n","            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n","\n","            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n","            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n","\n","            # Calculate the true label loss\n","            label_loss = ce_loss(student_logits, labels)\n","\n","            # Weighted sum of the two losses\n","            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n","\n","# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n","train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=20, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n","test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n","\n","# Compare the student test accuracy with and without the teacher, after distillation\n","print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n","print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n","print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"],"metadata":{"id":"Nec7zeu_qPOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757986500757,"user_tz":-540,"elapsed":252044,"user":{"displayName":"김광수","userId":"04260902725982947063"}},"outputId":"a7deaf87-98e9-4684-db46-066ecb0656f7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 1.9071680118360788\n","Epoch 2/20, Loss: 1.6991814335288904\n","Epoch 3/20, Loss: 1.5504554735730067\n","Epoch 4/20, Loss: 1.4227613733552607\n","Epoch 5/20, Loss: 1.307835461538466\n","Epoch 6/20, Loss: 1.2031545661904317\n","Epoch 7/20, Loss: 1.1121375883936577\n","Epoch 8/20, Loss: 1.0264039713403452\n","Epoch 9/20, Loss: 0.9439729860676523\n","Epoch 10/20, Loss: 0.8704827257129543\n","Epoch 11/20, Loss: 0.7943632607264897\n","Epoch 12/20, Loss: 0.7324981783204676\n","Epoch 13/20, Loss: 0.6728751817170311\n","Epoch 14/20, Loss: 0.6200400577939075\n","Epoch 15/20, Loss: 0.5725565247828394\n","Epoch 16/20, Loss: 0.5301408455195025\n","Epoch 17/20, Loss: 0.49179319743914984\n","Epoch 18/20, Loss: 0.4584058919526122\n","Epoch 19/20, Loss: 0.42822205540164354\n","Epoch 20/20, Loss: 0.406745734498324\n","Test Accuracy: 69.22%\n","Teacher accuracy: 75.57%\n","Student accuracy without teacher: 69.11%\n","Student accuracy with CE + KD: 69.22%\n"]}]},{"cell_type":"code","source":["class ModifiedDeepNNCosine(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ModifiedDeepNNCosine, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_classes)\n","        )\n","    def forward(self, x):\n","        x = self.features(x)\n","        flattened_conv_output = torch.flatten(x, 1)\n","        x = self.classifier(flattened_conv_output)\n","        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n","        return x, flattened_conv_output_after_pooling\n","\n","# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n","class ModifiedLightNNCosine(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ModifiedLightNNCosine, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(1024, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(256, num_classes)\n","        )\n","    def forward(self, x):\n","        x = self.features(x)\n","        flattened_conv_output = torch.flatten(x, 1)\n","        x = self.classifier(flattened_conv_output)\n","        return x, flattened_conv_output\n","\n","# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n","modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n","modified_nn_deep.load_state_dict(nn_deep.state_dict())\n","\n","# Once again ensure the norm of the first layer is the same for both networks\n","print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n","print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n","\n","# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n","torch.manual_seed(42)\n","modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n","print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())\n","\n","torch.manual_seed(42)\n","modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n","print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"],"metadata":{"id":"wYny3pLgqDEQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757986523391,"user_tz":-540,"elapsed":83,"user":{"displayName":"김광수","userId":"04260902725982947063"}},"outputId":"63f033fd-4986-4efa-d6c5-3c2222b2c05b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Norm of 1st layer for deep_nn: 7.753559589385986\n","Norm of 1st layer for modified_deep_nn: 7.753559589385986\n","Norm of 1st layer: 2.327361822128296\n","Norm of 1st layer: 2.327361822128296\n"]}]},{"cell_type":"code","source":["# Create a sample input tensor\n","sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n","\n","# Pass the input through the student\n","logits, hidden_representation = modified_nn_light(sample_input)\n","\n","# Print the shapes of the tensors\n","print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n","print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n","\n","# Pass the input through the teacher\n","logits, hidden_representation = modified_nn_deep(sample_input)\n","\n","# Print the shapes of the tensors\n","print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n","print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRxcXPLADfMu","executionInfo":{"status":"ok","timestamp":1757986528095,"user_tz":-540,"elapsed":6,"user":{"displayName":"김광수","userId":"04260902725982947063"}},"outputId":"23aace36-2dd9-4281-d96f-881398a5a6ff"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Student logits shape: torch.Size([128, 10])\n","Student hidden representation shape: torch.Size([128, 1024])\n","Teacher logits shape: torch.Size([128, 10])\n","Teacher hidden representation shape: torch.Size([128, 1024])\n"]}]},{"cell_type":"code","source":["def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n","    ce_loss = nn.CrossEntropyLoss()\n","    cosine_loss = nn.CosineEmbeddingLoss()\n","    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n","\n","    teacher.to(device)\n","    student.to(device)\n","    teacher.eval()  # Teacher set to evaluation mode\n","    student.train() # Student to train mode\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass with the teacher model and keep only the hidden representation\n","            with torch.no_grad():\n","                _, teacher_hidden_representation = teacher(inputs)\n","\n","            # Forward pass with the student model\n","            student_logits, student_hidden_representation = student(inputs)\n","\n","            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n","            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n","\n","            # Calculate the true label loss\n","            label_loss = ce_loss(student_logits, labels)\n","\n","            # Weighted sum of the two losses\n","            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"],"metadata":{"id":"Ml0dijRxDj96","executionInfo":{"status":"ok","timestamp":1757985481374,"user_tz":-540,"elapsed":2,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def test_multiple_outputs(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Test Accuracy: {accuracy:.2f}%\")\n","    return accuracy"],"metadata":{"id":"rnKpNFTBDqQL","executionInfo":{"status":"ok","timestamp":1757986543042,"user_tz":-540,"elapsed":4,"user":{"displayName":"김광수","userId":"04260902725982947063"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Train and test the lightweight network with cross entropy loss\n","train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=20, learning_rate=0.001,\n","                  hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n","test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4xF4owaGf8M","executionInfo":{"status":"ok","timestamp":1757986809690,"user_tz":-540,"elapsed":258976,"user":{"displayName":"김광수","userId":"04260902725982947063"}},"outputId":"a849f2ed-bf22-43bb-fa64-8ff5236d711e"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 1.308491258669997\n","Epoch 2/20, Loss: 1.0721526258741803\n","Epoch 3/20, Loss: 0.9723540135966543\n","Epoch 4/20, Loss: 0.8976628324564766\n","Epoch 5/20, Loss: 0.8460214583160323\n","Epoch 6/20, Loss: 0.8008004228782166\n","Epoch 7/20, Loss: 0.7577904174699808\n","Epoch 8/20, Loss: 0.7235273083152673\n","Epoch 9/20, Loss: 0.6873242481590232\n","Epoch 10/20, Loss: 0.6602568071516578\n","Epoch 11/20, Loss: 0.6303940819352484\n","Epoch 12/20, Loss: 0.6021087020254501\n","Epoch 13/20, Loss: 0.5730948635684255\n","Epoch 14/20, Loss: 0.5505662844766437\n","Epoch 15/20, Loss: 0.5196736778139763\n","Epoch 16/20, Loss: 0.4983766588865948\n","Epoch 17/20, Loss: 0.47957804074982546\n","Epoch 18/20, Loss: 0.46000341869071315\n","Epoch 19/20, Loss: 0.4416675258932821\n","Epoch 20/20, Loss: 0.4236981900756621\n","Test Accuracy: 71.17%\n"]}]}]}