{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18f7c79",
   "metadata": {},
   "source": [
    "# 0904 인공지능 및 딥러닝 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de6503",
   "metadata": {},
   "source": [
    "## 1. 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b14e",
   "metadata": {},
   "source": [
    "`-` 뇌과학 관점\n",
    "\n",
    "* Neural Network\n",
    "* 인간의 뇌를 모방"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff470c",
   "metadata": {},
   "source": [
    "`-` 공학적 관점\n",
    "\n",
    "* XOR 문제 해결 : Linear-Perceptron으로는 해결 불가\n",
    "> Non-Linear 형태로 풀이하여 해결 - Multi-Layer-Perceptron\n",
    "* 통계학에서의 비선형 모델링 $y = f(x) + \\epsilon$ -> $f$의 설계\n",
    "* MLP $f = g_2 \\circ g_1$ : 미분 어려움, 해석에서 문제 있음\n",
    "> Complexity 훨씬 높음, 다양한 문제에 적용 가능 $\\to$ DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de310731",
   "metadata": {},
   "source": [
    "## 2. DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e11975",
   "metadata": {},
   "source": [
    "`-` MLP 정의$(2 \\times 2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76dd99c",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a530a2f",
   "metadata": {},
   "source": [
    "> 단순 레이어 변환. 선형 변환임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bf162",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\overset{\\sigma}{\\to} \\begin{bmatrix} \\sigma (h_1) \\\\ \\sigma (h_2) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698eecc9",
   "metadata": {},
   "source": [
    "> ReLU, sigmoid 등의 비선형 변환을 추가하여 복잡한 형태를 만들어냄. $W$의 차원은 자유롭게 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76be44f",
   "metadata": {},
   "source": [
    "$$f({\\boldsymbol x}) = a(g_L \\circ g_{L-1} \\circ \\cdots \\circ g_1({\\boldsymbol x}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb232776",
   "metadata": {},
   "source": [
    "`-` Loss Function $l(y,f(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac72b9",
   "metadata": {},
   "source": [
    "> 일반적으로 Convex Function을 가정\n",
    ">\n",
    "> softmax loss(CrossEntropy), MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da5cfb",
   "metadata": {},
   "source": [
    "* $f(x)$의 형태가 이미 정해져 있다면, loss function은 convex함. 하지만 $f$의 형태를 바꾸는 $\\theta : x \\to f$도 파악해야 함. convex하지 않아 최적화가 어려움\n",
    "* convex가 되면 미분해서 0이 되는 지점만 구하면 됨, non-convex하다면 미분해서 0이 되는 포인트가 많아짐\n",
    "* $\\theta$의 비중이 압도적으로 큼. Feature extractor가 중요한 정보를 가지고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92588663",
   "metadata": {},
   "source": [
    "`-` 인간의 뉴런과 DNN의 차이\n",
    "\n",
    "* Activation : 전기 신호 $\\approx$ 활성화 함수\n",
    "> 인간의 뇌의 시냅스 연결 - 무작위적, 비정형\n",
    ">\n",
    "> DNN의 퍼셉트론 간 연결 - 선형적?, 정형\n",
    "\n",
    "* 역전파\n",
    "> 인간의 뇌 - 전체를 바꾸지 않고 일부만 변경하는 것이 가능\n",
    ">\n",
    "> DNN - 개별 weight만 바꿀 수 없음. 모든 뉴런을 다 변경해야 함 $\\to$ 비효율적, complexity 제약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987a115",
   "metadata": {},
   "source": [
    "`-` CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c394b",
   "metadata": {},
   "source": [
    "* 전체 이미지에서 엣지를 찾지 않고, 개별 커널에서 엣지 탐색 수 convolution으로 통합 $\\to$ 반복을 통해 엣지 정보를 추출 : characterize\n",
    "> 이미지 처리, 인식에 대해서 뉴럴 네트워크가 더 잘 이해할 수 있도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193ff2e",
   "metadata": {},
   "source": [
    "`-` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9541a",
   "metadata": {},
   "source": [
    "* Swallow NN : 히든 레이어를 적게\n",
    "\n",
    "$$\\text{input layer} \\to h_1 \\to h_2 \\to \\text{output layer}$$\n",
    "\n",
    "* Deep NN : 히든 레이어가 3개 이상... 다 말이 다르네\n",
    "\n",
    "$$\\text{input layer} \\to h_1 \\to h_2 \\to h_3 \\to \\cdots \\to \\text{output layer}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ab33c",
   "metadata": {},
   "source": [
    "* 미분값 소실 : activate function $\\sigma'(\\cdot)$의 미분값 비율만큼 레이어마다 그래디언트가 감소 $\\to$ vanishing\n",
    "> ReLU로 풀면 그래디언트 잘 안사라짐 ㅇㅇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd0e9",
   "metadata": {},
   "source": [
    "* batch normalization : 계수가 너무 크면 곱해지면서 폭발 $\\to$ 정규화 필요\n",
    "\n",
    "* dropout : 인간의 뇌가 랜덤하게 connection되는 것을 모방했다고 말할 수 있음 $\\to$ 학습이 부드러워짐\n",
    "\n",
    "* regularization : 모수 값을 너무 크지 않게, l1/l2 norm 등 사용\n",
    "\n",
    "* skip connection : 레이어 연결 과정에서 스킵하여 그래디언트 계산이 쉽도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9ed04",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent : 랜덤 배치를 사용하는 경사 하강법\n",
    "\n",
    "> loss를 내리기는 어려우나, 어느 구간을 넘어간다면 flat 구간에 도착할 것이다 $\\to$ 안정적인 구간, loss가 가장 작은 곳을 가지는 구간\n",
    ">\n",
    "> 맞나? $\\to$ 경험적으로 많이 나옴 + 해당 상황을 조작하여 만들어낼 수 있음"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
