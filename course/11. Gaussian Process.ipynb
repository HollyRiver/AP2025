{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f574f",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b2df5",
   "metadata": {},
   "source": [
    "## 1. Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa335e",
   "metadata": {},
   "source": [
    "* ML에서 가장 성능이 좋\"았\"던 모델\n",
    "* DNN과 관련이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e508611",
   "metadata": {},
   "source": [
    "`-` Process\n",
    "\n",
    "* 시간에 따라 변화하는 벡터, 공간, 함수\n",
    "* GP: 함수 공간의 random process. random function으로도 불림\n",
    "* 알고 싶은 함수 자체를 알아내는 문제. Prior/Posterior를 함수 자체로 설정\n",
    "\n",
    "> 함수는 각 값들이 연관되어 있음 $\\to$ k차 미분 가능성으로 표시\n",
    ">\n",
    "> 고차원 미분이 가능해질 수록 주변값들과의 연관성이 강해짐\n",
    "\n",
    "* 알아내고자 하는 함수 자체가 최소한 연속은 되는 형태의 함수이다. 모든 포인트가 서로 독립적이면 안됨. correlation이 존재하는 process로 모델링을 해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fc568",
   "metadata": {},
   "source": [
    "`-` Example of a random process\n",
    "\n",
    "* $\\kappa = \\exp \\left(\\frac{-||x-x'||^2}{2t^2}\\right)$\n",
    "* $\\kappa = \\min (x, x')$\n",
    "* $\\kappa = (x^{\\top}x' + c)^2$\n",
    "\n",
    "$\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d11879",
   "metadata": {},
   "source": [
    "`-` Continuous stochastic process (시간을 불가산 무한으로 취급)\n",
    "\n",
    "* 확률은 기본적으로 최소한 가산 무한을 가정해야 함 $\\to$ 불가산 무한을 가산 무한으로 projection.\n",
    "\n",
    "* Notation: $\\mathcal G \\mathcal P (\\boldsymbol \\mu, k)$\n",
    "\n",
    "* For any finite $(x_1, \\cdots, x_N)$ on $\\chi$\n",
    "\n",
    "$$(f(x_1), \\cdots, f(x_N)) \\sim \\mathcal N((\\mu_1, \\cdots, \\mu_N)^{\\top}, \\boldsymbol \\Sigma_N)$$\n",
    "\n",
    "* $\\boldsymbol \\mu$는 mean function. 랜덤하게 만들어지는 값들 중에 평균이 존재한다는 것임.\n",
    "* $\\boldsymbol \\Sigma_N = \\{ k(x_i, x_j) \\}_{(i, j)\\in [N]\\times [N]}$: 커널이 공분산 행렬을 생산함. $\\text{Cov}(x_i, x_j) = k(x_i, x_j)$\n",
    "> process 관점에서 $x_i, x_j$가 가까워질수록 공분산의 절대값은 커져야 한다. 상관성이 강해야 함. (corr의 절대값이 1에 가까워짐)\n",
    ">\n",
    "> $\\mu, k$만 존재하면 된다. $\\mu$는 mean function으로, prior와 비슷하여 $0$으로 설정하면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d8a6a",
   "metadata": {},
   "source": [
    "`-` $k$의 설정\n",
    "\n",
    "* 커널을 통해 공분산을 만들었을 때, 이론적으로 합당해야 함\n",
    "\n",
    "1. Positive semi-definite.\n",
    "\n",
    "$$\\sum_i \\sum_j c_i c_j k(x_i, x_j) \\geq 0 ~ \\text{where} ~ x_i \\in \\chi, c_i \\in \\mathbb R$$\n",
    "\n",
    "2. Consistency\n",
    "\n",
    "$$p(f(x_1)) = \\int p(f(x_1), f(x_2)) df(x_2)$$\n",
    "\n",
    "> 모든 $x_1, x_2$에서 위 수식이 성립해야만 함.\n",
    ">\n",
    "> 이는 GP의 definition에서 유도된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7b404",
   "metadata": {},
   "source": [
    "`-` kernel $k$\n",
    "\n",
    "* Prior의 설정이 잘못되면, likelihood가 들어와도 잘못된 결과가 나올 수 있다.\n",
    "\n",
    "1. $\\boldsymbol K_{ij} = k(x_i, x_j)$는 $f(x_i), f(x_j)$의 공분산이다.\n",
    "2. 다양한 커널이 예측 성질에 차이를 줌\n",
    "3. 다양한 커널 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b09fb",
   "metadata": {},
   "source": [
    "## 2. Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ef073",
   "metadata": {},
   "source": [
    "`-` Squared Exponential (SE) covariance (RBF kernel)\n",
    "\n",
    "$$k(x, x') = \\sigma^2_0 \\exp \\left( -\\frac{1}{2\\lambda}||x - x'||^2 \\right)$$\n",
    "\n",
    "1.\n",
    "2. $\\sigma_0, \\lambda$는 하이퍼파라미터이다.\n",
    "3. 해당 커널은 매우 부드러움 모양의 함수를 생산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c261f",
   "metadata": {},
   "source": [
    "`-` Matern kernel\n",
    "\n",
    "1. $K_{\\nu}$는 Bessel function으로 결정됨. $\\nu$에 따라서 형태가 많이 달라진다.\n",
    "2. $\\nu = 1/2$일 때, 더 러프한 샘플이 나온다.\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47cb637",
   "metadata": {},
   "source": [
    "`-` Non-stationary kernels\n",
    "\n",
    "1. Linear kernel: $k(x, x') = \\sigma^2_0 + x^{\\top}x'$\n",
    "> mean에서 벗어날 수록 매우 달라짐. 변동 심해짐\n",
    "2. Brownian motion: $k(x, x') = \\min(x_1, x_2)$ where $x_1, x_2 \\in \\mathbb R$\n",
    "> Random walk\n",
    "3. Periodic kernel: $k(x, x') = \\exp \\left( - \\frac{2\\lambda \\sin^2((x - x')/2)}{\\lambda^2} \\right)$\n",
    "> 주기성을 고려\n",
    "4. Neural tangent kernel\n",
    "> DNN을 GP로 approximation할 때 사용할 수 있는 일반적인 커널. Gradient를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421852f5",
   "metadata": {},
   "source": [
    "`-` Combination of kernel for a new kernel\n",
    "\n",
    "1. Sum: 단순합\n",
    "2. Product: 단순곱\n",
    "3. Convolution: $\\int h(x, z)k(z, z')h(z', x)dzdz'$\n",
    "> Convolution까지는 잘 활용할 필요가 없고, Sum과 Product만으로 충분함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3a58b",
   "metadata": {},
   "source": [
    "## 3. (Bayesian) Prediction with GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9118d1",
   "metadata": {},
   "source": [
    "`-` Multivatiate Normal distribution\n",
    "\n",
    "* Normal은 약간 closed distribution임. 결합, 주변, 조건부 전부 정규분포임\n",
    "\n",
    "$$\\boldsymbol y = (\\boldsymbol y_1^{\\top}, \\boldsymbol y_2^{\\top})^{\\top} \\sim \\mathcal N \\left( (\\boldsymbol \\mu_1^{\\top}, \\boldsymbol \\mu_2^{\\top})^{\\top}, \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix} \\right)$$\n",
    "\n",
    "$$\\boldsymbol y_2 ~|~ \\boldsymbol y_1 \\sim \\mathcal N \\left( \\boldsymbol \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1}(\\boldsymbol y_1 - \\boldsymbol \\mu_1) \\right)$$\n",
    "\n",
    "* $f_i$들이 관측되었다고 가정할 때, 관측되지 않은 $f^*$의 분포를 파악할 수 있음. 이와 같은 형태로 Prediction...\n",
    "* $f_i$는 전부 평균만 얘기하기 때문에... 실제로 $\\epsilon$이 들어왔을 때 수식이 좀 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391c8ef",
   "metadata": {},
   "source": [
    "`-` 다변량 정규분포와 회귀분석과의 관계\n",
    "\n",
    "Let $\\mu_1, \\mu_2 = 0, ~ y_2 = y, y_1 = x$.\n",
    "\n",
    "$$\\begin{align}\n",
    "& X: X^{\\top}X \\\\\n",
    "& Y: Y^{\\top}X ~ \\text{or} ~ X^{\\top}Y\n",
    "\\end{align}$$\n",
    "\n",
    "> 내적으로 공분산 행렬 유도 가능\n",
    "\n",
    "* $(X^{\\top}X)^{-1}X^{\\top}Y$ 계산 시 $Y$의 일부 값이 누락된 semi-supervised learning 문제에서 레이블이 없는 $X$를 전부 사용해도 되지 않을까란 생각\n",
    "* $X^{\\top}X$부분의 $X$와 $X^{\\top}Y$부분의 $X$를 다르게."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ea0dd",
   "metadata": {},
   "source": [
    "`-` Prior\n",
    "\n",
    "* Likelihood의 함수에 적용. function space로 지정됨\n",
    "* Regression: $y$는 평균만 관찰되는 것이 아니고, $\\epsilon$이 같이 관측됨\n",
    "* Classification: $y$의 확률을 계산하는 함수를 계산, 해당 함수에 Prior를 지정\n",
    "> Regression은 Gaussian으로 한정지을 수 있으나, Classification은 Gaussian으로 가정할 수 없음. 다른 방법이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51d04b",
   "metadata": {},
   "source": [
    "`-` Regression with noise\n",
    "\n",
    "1. $y = f + \\epsilon$\n",
    "2. Likelihood: $p(y_{1:N}~|~f_{1:N}) = N(y_{1:N}~|~f_{1:N}, \\sigma^2 I)$\n",
    "3. Marginal likelihood: $p(y_{1:N}) = N(y_{1:N}~|~\\mu, K_{N:N} + \\sigma^2 I)$\n",
    "\n",
    "> $\\epsilon_i$는 상호 독립으로 모델링했고, $k$는 covariance matrix를 구성하는 인자이므로, Marginal likelihood는 분산에 해당하는 대각원소에만 $\\sigma^2$를 더해준 형태로 공분산 행렬을 가진다.\n",
    ">\n",
    "> 비대각원소는 $k_{ij}$와 $(K + \\sigma^2 I)_{ij}$의 값이 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b7d7c",
   "metadata": {},
   "source": [
    "`-` GP posterior: for any $x \\in \\mathcal X$\n",
    "\n",
    "$$p(y^*~|~y) = N(y^*~|~\\mu^{\\text{post}}, \\Sigma^{\\text{post}})$$\n",
    "\n",
    "where $\\mu^{\\text{post}} = \\mu_{1:m}^* + K_{m:N}(K + \\sigma^2 I)^{-1}_{N:N}(y - \\mu)$ and $\\Sigma^{\\text{post}} = K_{m:m} - K_{m:N}(K_{N:N} + \\sigma^2I)^{-1} K_{N:m} + \\sigma^2$\n",
    "\n",
    "1. Posterior mean has the form of $\\sum_{i=1}^N \\alpha_i y_i$. 관측값의 선형결합으로 (weighted sum) 최종 예측값을 산출\n",
    "2. Calculations cost is $O(N^3)$ due to the inversion\n",
    "3. In fact, the posterior is also a GP.\n",
    "\n",
    "> 원래 전체를 해야 하지만, 소수의 데이터셋만으로 전체의 효율을 낼 수 있도록 Variational Bayes를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3c9a6",
   "metadata": {},
   "source": [
    "`-` DNN\n",
    "\n",
    "* Deep Gaussian Process DGP: GP로 모델 $f(x)$를 구성한 다음, given $f$에 대하여 conditional gaussian process $g$를 다시 재구성. $g(f(x))$ ... 이를 반복해서 깊게 중첩하면 새로운 결과가 나오지 않을?까\n",
    "* 특정한 형태의 DNN을 구성한 뒤, 그것을 GP로 근사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cad649",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
