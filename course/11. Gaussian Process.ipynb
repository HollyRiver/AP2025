{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f574f",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b2df5",
   "metadata": {},
   "source": [
    "## 1. Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa335e",
   "metadata": {},
   "source": [
    "* ML에서 가장 성능이 좋\"았\"던 모델\n",
    "* DNN과 관련이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e508611",
   "metadata": {},
   "source": [
    "`-` Process\n",
    "\n",
    "* 시간에 따라 변화하는 벡터, 공간, 함수\n",
    "* GP: 함수 공간의 random process. random function으로도 불림\n",
    "* 알고 싶은 함수 자체를 알아내는 문제. Prior/Posterior를 함수 자체로 설정\n",
    "\n",
    "> 함수는 각 값들이 연관되어 있음 $\\to$ k차 미분 가능성으로 표시\n",
    ">\n",
    "> 고차원 미분이 가능해질 수록 주변값들과의 연관성이 강해짐\n",
    "\n",
    "* 알아내고자 하는 함수 자체가 최소한 연속은 되는 형태의 함수이다. 모든 포인트가 서로 독립적이면 안됨. correlation이 존재하는 process로 모델링을 해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fc568",
   "metadata": {},
   "source": [
    "`-` Example of a random process\n",
    "\n",
    "* $\\kappa = \\exp \\left(\\frac{-||x-x'||^2}{2t^2}\\right)$\n",
    "* $\\kappa = \\min (x, x')$\n",
    "* $\\kappa = (x^{\\top}x' + c)^2$\n",
    "\n",
    "$\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d11879",
   "metadata": {},
   "source": [
    "`-` Continuous stochastic process (시간을 불가산 무한으로 취급)\n",
    "\n",
    "* 확률은 기본적으로 최소한 가산 무한을 가정해야 함 $\\to$ 불가산 무한을 가산 무한으로 projection.\n",
    "\n",
    "* Notation: $\\mathcal G \\mathcal P (\\boldsymbol \\mu, k)$\n",
    "\n",
    "* For any finite $(x_1, \\cdots, x_N)$ on $\\chi$\n",
    "\n",
    "$$(f(x_1), \\cdots, f(x_N)) \\sim \\mathcal N((\\mu_1, \\cdots, \\mu_N)^{\\top}, \\boldsymbol \\Sigma_N)$$\n",
    "\n",
    "* $\\boldsymbol \\mu$는 mean function. 랜덤하게 만들어지는 값들 중에 평균이 존재한다는 것임.\n",
    "* $\\boldsymbol \\Sigma_N = \\{ k(x_i, x_j) \\}_{(i, j)\\in [N]\\times [N]}$: 커널이 공분산 행렬을 생산함. $\\text{Cov}(x_i, x_j) = k(x_i, x_j)$\n",
    "> process 관점에서 $x_i, x_j$가 가까워질수록 공분산의 절대값은 커져야 한다. 상관성이 강해야 함. (corr의 절대값이 1에 가까워짐)\n",
    ">\n",
    "> $\\mu, k$만 존재하면 된다. $\\mu$는 mean function으로, prior와 비슷하여 $0$으로 설정하면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d8a6a",
   "metadata": {},
   "source": [
    "`-` $k$의 설정\n",
    "\n",
    "* 커널을 통해 공분산을 만들었을 때, 이론적으로 합당해야 함\n",
    "\n",
    "1. Positive semi-definite.\n",
    "\n",
    "$$\\sum_i \\sum_j c_i c_j k(x_i, x_j) \\geq 0 ~ \\text{where} ~ x_i \\in \\chi, c_i \\in \\mathbb R$$\n",
    "\n",
    "2. Consistency\n",
    "\n",
    "$$p(f(x_1)) = \\int p(f(x_1), f(x_2)) df(x_2)$$\n",
    "\n",
    "> 모든 $x_1, x_2$에서 위 수식이 성립해야만 함.\n",
    ">\n",
    "> 이는 GP의 definition에서 유도된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7b404",
   "metadata": {},
   "source": [
    "`-` kernel $k$\n",
    "\n",
    "* Prior의 설정이 잘못되면, likelihood가 들어와도 잘못된 결과가 나올 수 있다.\n",
    "\n",
    "1. $\\boldsymbol K_{ij} = k(x_i, x_j)$는 $f(x_i), f(x_j)$의 공분산이다.\n",
    "2. 다양한 커널이 예측 성질에 차이를 줌\n",
    "3. 다양한 커널 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b09fb",
   "metadata": {},
   "source": [
    "## 2. Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ef073",
   "metadata": {},
   "source": [
    "`-` Squared Exponential (SE) covariance (RBF kernel)\n",
    "\n",
    "$$k(x, x') = \\sigma^2_0 \\exp \\left( -\\frac{1}{2\\lambda}||x - x'||^2 \\right)$$\n",
    "\n",
    "1.\n",
    "2. $\\sigma_0, \\lambda$는 하이퍼파라미터이다.\n",
    "3. 해당 커널은 매우 부드러움 모양의 함수를 생산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c261f",
   "metadata": {},
   "source": [
    "`-` Matern kernel\n",
    "\n",
    "1. $K_{\\nu}$는 Bessel function으로 결정됨. $\\nu$에 따라서 형태가 많이 달라진다.\n",
    "2. $\\nu = 1/2$일 때, 더 러프한 샘플이 나온다.\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47cb637",
   "metadata": {},
   "source": [
    "`-` Non-stationary kernels\n",
    "\n",
    "1. Linear kernel: $k(x, x') = \\sigma^2_0 + x^{\\top}x'$\n",
    "> mean에서 벗어날 수록 매우 달라짐. 변동 심해짐\n",
    "2. Brownian motion: $k(x, x') = \\min(x_1, x_2)$ where $x_1, x_2 \\in \\mathbb R$\n",
    "> Random walk\n",
    "3. Periodic kernel: $k(x, x') = \\exp \\left( - \\frac{2\\lambda \\sin^2((x - x')/2)}{\\lambda^2} \\right)$\n",
    "> 주기성을 고려\n",
    "4. Neural tangent kernel\n",
    "> DNN을 GP로 approximation할 때 사용할 수 있는 일반적인 커널. Gradient를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421852f5",
   "metadata": {},
   "source": [
    "`-` Combination of kernel for a new kernel\n",
    "\n",
    "1. Sum: 단순합\n",
    "2. Product: 단순곱\n",
    "3. Convolution: $\\int h(x, z)k(z, z')h(z', x)dzdz'$\n",
    "> Convolution까지는 잘 활용할 필요가 없고, Sum과 Product만으로 충분함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3a58b",
   "metadata": {},
   "source": [
    "## 3. (Bayesian) Prediction with GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9118d1",
   "metadata": {},
   "source": [
    "`-` Multivatiate Normal distribution\n",
    "\n",
    "* Normal은 약간 closed distribution임. 결합, 주변, 조건부 전부 정규분포임\n",
    "\n",
    "$$\\boldsymbol y = (\\boldsymbol y_1^{\\top}, \\boldsymbol y_2^{\\top})^{\\top} \\sim \\mathcal N \\left( (\\boldsymbol \\mu_1^{\\top}, \\boldsymbol \\mu_2^{\\top})^{\\top}, \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix} \\right)$$\n",
    "\n",
    "$$\\boldsymbol y_2 ~|~ \\boldsymbol y_1 \\sim \\mathcal N \\left( \\boldsymbol \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1}(\\boldsymbol y_1 - \\boldsymbol \\mu_1) \\right)$$\n",
    "\n",
    "* $f_i$들이 관측되었다고 가정할 때, 관측되지 않은 $f^*$의 분포를 파악할 수 있음. 이와 같은 형태로 Prediction...\n",
    "* $f_i$는 전부 평균만 얘기하기 때문에... 실제로 $\\epsilon$이 들어왔을 때 수식이 좀 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391c8ef",
   "metadata": {},
   "source": [
    "`-` 다변량 정규분포와 회귀분석과의 관계\n",
    "\n",
    "Let $\\mu_1, \\mu_2 = 0, ~ y_2 = y, y_1 = x$.\n",
    "\n",
    "$$\\begin{align}\n",
    "& X: X^{\\top}X \\\\\n",
    "& Y: Y^{\\top}X ~ \\text{or} ~ X^{\\top}Y\n",
    "\\end{align}$$\n",
    "\n",
    "> 내적으로 공분산 행렬 유도 가능\n",
    "\n",
    "* $(X^{\\top}X)^{-1}X^{\\top}Y$ 계산 시 $Y$의 일부 값이 누락된 semi-supervised learning 문제에서 레이블이 없는 $X$를 전부 사용해도 되지 않을까란 생각\n",
    "* $X^{\\top}X$부분의 $X$와 $X^{\\top}Y$부분의 $X$를 다르게."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ea0dd",
   "metadata": {},
   "source": [
    "`-` Prior\n",
    "\n",
    "* Likelihood의 함수에 적용. function space로 지정됨\n",
    "* Regression: $y$는 평균만 관찰되는 것이 아니고, $\\epsilon$이 같이 관측됨\n",
    "* Classification: $y$의 확률을 계산하는 함수를 계산, 해당 함수에 Prior를 지정\n",
    "> Regression은 Gaussian으로 한정지을 수 있으나, Classification은 Gaussian으로 가정할 수 없음. 다른 방법이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51d04b",
   "metadata": {},
   "source": [
    "`-` Regression with noise\n",
    "\n",
    "1. $y = f + \\epsilon$\n",
    "2. Likelihood: $p(y_{1:N}~|~f_{1:N}) = N(y_{1:N}~|~f_{1:N}, \\sigma^2 I)$\n",
    "3. Marginal likelihood: $p(y_{1:N}) = N(y_{1:N}~|~\\mu, K_{N:N} + \\sigma^2 I)$\n",
    "\n",
    "> $\\epsilon_i$는 상호 독립으로 모델링했고, $k$는 covariance matrix를 구성하는 인자이므로, Marginal likelihood는 분산에 해당하는 대각원소에만 $\\sigma^2$를 더해준 형태로 공분산 행렬을 가진다.\n",
    ">\n",
    "> 비대각원소는 $k_{ij}$와 $(K + \\sigma^2 I)_{ij}$의 값이 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b7d7c",
   "metadata": {},
   "source": [
    "`-` GP posterior: for any $x \\in \\mathcal X$\n",
    "\n",
    "$$p(y^*~|~y) = N(y^*~|~\\mu^{\\text{post}}, \\Sigma^{\\text{post}})$$\n",
    "\n",
    "where $\\mu^{\\text{post}} = \\mu_{1:m}^* + K_{m:N}(K + \\sigma^2 I)^{-1}_{N:N}(y - \\mu)$ and $\\Sigma^{\\text{post}} = K_{m:m} - K_{m:N}(K_{N:N} + \\sigma^2I)^{-1} K_{N:m} + \\sigma^2$\n",
    "\n",
    "1. Posterior mean has the form of $\\sum_{i=1}^N \\alpha_i y_i$. 관측값의 선형결합으로 (weighted sum) 최종 예측값을 산출\n",
    "2. Calculations cost is $O(N^3)$ due to the inversion\n",
    "3. In fact, the posterior is also a GP.\n",
    "\n",
    "> 원래 전체를 해야 하지만, 소수의 데이터셋만으로 전체의 효율을 낼 수 있도록 Variational Bayes를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3c9a6",
   "metadata": {},
   "source": [
    "`-` DNN\n",
    "\n",
    "* Deep Gaussian Process DGP: GP로 모델 $f(x)$를 구성한 다음, given $f$에 대하여 conditional gaussian process $g$를 다시 재구성. $g(f(x))$ ... 이를 반복해서 깊게 중첩하면 새로운 결과가 나오지 않을?까\n",
    "* 특정한 형태의 DNN을 구성한 뒤, 그것을 GP로 근사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cad649",
   "metadata": {},
   "source": [
    "### 내용 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1f5af",
   "metadata": {},
   "source": [
    "* Process: 함수 자체를 모델링하겠다. Prior를 Process로 잡겠다.\n",
    "* 어떤 kernel을 선택하는지에 따라 함수의 개형이 크게 달라진다. 그런 관점에서 연속이기만 한 Brownian motion은 잘 쓰지 않는다.\n",
    "* kernel의 값들이 관측치의 covariance를 결정해준다. 커널은 semi-definite여야 한다.\n",
    "\n",
    "* RBF kernel: 무한 미분 가능. GT가 어떤 것이든지 부드럽게 이으려고 노력함... 일반화 잘됨\n",
    "* Matern kernel: 미분을 한정지음. 특정 데이터에서 성능 좋을 수 있음\n",
    "* Non-stationary kernels: Periodic kernel을 잘 쓰면 시계열 분석 잘 할 수 있음\n",
    "\n",
    "* 사실상 GP는 커널이 전부. 모든 데이터에 잘 되는 커널은 없고, RBF는 그냥 얼추 잘 되는 거고, 데이터에 따라 커널을 다 달리 사용해야 함\n",
    "\n",
    "* 커널 결합은 Sum / Product / Convolution이 있지만 Sum & Product만 써도 잘 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3d9cf",
   "metadata": {},
   "source": [
    "* Regression에서는 Noise가 존재함에 따라 커널의 형태가 바뀜. 대각원소에 노이즈 분산 추가\n",
    "* given data $x$가 $y$와의 상관성이 높을수록 $y|x$의 분산이 줄어든다. 즉, 피쳐가 타겟과 상관성이 높을수록 예측에서의 분산이 줄어든다.\n",
    "\n",
    "$$y_2 ~|~ y_1 \\sim N(\\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(y  -\\mu_1), \\Sigma_{21} \\Sigma_{11}^{-1}\\Sigma_{12})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85531c80",
   "metadata": {},
   "source": [
    "* GP posterior 계산의 비용은 역행렬을 계산해야 되므로 데이터 개수에 따라 $O(N^3)$에 해당한다. DNN은 $O(N)$이므로 데이터셋이 커져도 문제가 없다.\n",
    "* 따라서 데이터가 매우 많을 경우 일부만 사용하여 잘 할 수 있는 방법을 찾는 것이 현실적\n",
    "* 하이퍼파라미터 튜닝도 고려해야 함\n",
    "* GP는 Prior/Posterior 모두 GP임. 95% Credible Interval 내부에 Random Process가 있을 가능성이 95%\n",
    "* 데이터가 많아질수록 Credible Interval의 폭이 줄어듦. $y = f(x) + \\epsilon$으로 모델링된 경우 band의 넓이가 0이 될 수는 없음. $\\epsilon$ 때문에. $\\epsilon$이 없으면 band의 넓이는 0으로 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8c2d6",
   "metadata": {},
   "source": [
    "* MC Dropout의 Credibla Interval은 근사이지만, GP에서는 수학적으로 엄밀하게 증명된 것이므로 확실함\n",
    "* DNN의 width를 길게 만들수록 GP에 수렴\n",
    "> 학습이 잘 됨\n",
    ">\n",
    "> 예측값들이 smoothing되는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b92df",
   "metadata": {},
   "source": [
    "* Classification에서는 $y$가 연속형이 아니기 때문에 Posterior는 가우시안이 아님. non-conjugate\n",
    "* 가우시안이 아닌 애를 가우시안으로 억지로 만듦\n",
    "> Laplace approximation: 봉우리가 하나이고, maximum을 가지는 어떠한 분포라도 정규분포로 근사할 수 있다. mode와 2차 미분을 이용\n",
    ">\n",
    "> Expectation propagation: KL divergence를 이용해서 정규분포에 가장 가까운 애를 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f35cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
