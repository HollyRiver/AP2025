{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b470093a",
   "metadata": {},
   "source": [
    "`-` 질문 리스트\n",
    "\n",
    "* 타겟에서의 예측 문제 오타... 아닌가? $f: X_t \\to Y_t$ 아님?\n",
    "* pre-trained model을 사용할 때, 뒤쪽 MLP를 아예 떼어놓고 하는 경우가 많은데, 일부만 사용하는 것도 동일한 용어를 사용하면 되나?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381f223",
   "metadata": {},
   "source": [
    "## 1. AI에 대한 개괄적 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41668ec1",
   "metadata": {},
   "source": [
    "### **A. 인공지능**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ab4b6",
   "metadata": {},
   "source": [
    "명확하게 정리된 유일한 정의는 없음\n",
    "\n",
    "`-` **AI에 대한 기술적 정의 (Technical definition)**:\n",
    "* 인간의 오감과 관련된 정보를 통합하여 분석, 학습, 예측\n",
    "* 의사결정 수행\n",
    "* 새로운 분야 학습 시 이전 내용을 연결시켜 효율적으로 학습 가능: Continuous Learning\n",
    "* 언어 처리와 메타 러닝 등을 수행할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a73407",
   "metadata": {},
   "source": [
    "`-` 뇌과학 관점에서의 인공지능\n",
    "\n",
    "* Neural Network\n",
    "* 인간의 뇌를 모방"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434d988",
   "metadata": {},
   "source": [
    "`-` 공학적 관점에서의 인공지능\n",
    "\n",
    "* XOR 문제의 해결을 위한 Multi-Layer-Perceptron\n",
    "* 통계학에서의 비선형 모델링 $y = f(x) + \\epsilon$ -> $f$의 설계\n",
    "* MLP $f = g_2 \\circ g_1$ : 미분 어려움, 해석에서 문제 있음\n",
    "> Complexity 훨씬 높음, 다양한 문제에 적용 가능 $\\to$ DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a16d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "`-` Application Example\n",
    "\n",
    "* Precision Medicine: 의료의 개인화 Personalization\n",
    "* Virtual Assistant: 가상 비서\n",
    "* Expert System\n",
    "* Auto Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049e277",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "* 추론 Reasoning\n",
    "* 자율적인 의사결정 및 작업 수행 Agent\n",
    "* Physical AI\n",
    "* Computer Vision/Speech/Expert System은 이미 인간을 능가하였음\n",
    "* Continuous learning/Meta learning/Reinforcement learning에서 상당한 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b423a56",
   "metadata": {},
   "source": [
    "### **B. DNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91534ac8",
   "metadata": {},
   "source": [
    "`-` Simple DNN (Multi-Layer Perceptron; MLP)\n",
    "\n",
    "$$f({\\boldsymbol x}) = a(g_L \\circ g_{L-1} \\circ \\cdots \\circ g_1 ({\\boldsymbol x}))$$\n",
    "\n",
    "* $g_l(\\boldsymbol x) = \\sigma ({\\boldsymbol W_l \\boldsymbol x +  \\boldsymbol b_l})$이며, 해당 함수 $f$는 레이블 예측에 활용됨. 파라미터를 통해 학습되는 함수.\n",
    "* $\\sigma$는 활성화 함수(activation function)이며, $a(\\cdot) = {\\boldsymbol W_a ~ \\cdot} + {\\boldsymbol b_a}$는 스칼라 함수 또는 벡터 함수임(netout 벡터를 반환)\n",
    "\n",
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "\n",
    "> 단순 레이어 변환. 선형 변환임: $\\boldsymbol W_l  \\boldsymbol x$의 형태\n",
    "\n",
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\overset{\\sigma}{\\to} \\begin{bmatrix} \\sigma (h_1) \\\\ \\sigma (h_2) \\end{bmatrix}$$\n",
    "\n",
    "> 해당 선형 변환에 ReLU, sigmoid 등의 비선형 변환(activation function)을 추가하여 복잡한 형태를 만들어냄. $W$의 차원은 자유롭게 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ea660",
   "metadata": {},
   "source": [
    "`-` 인간의 뉴런과 DNN의 차이\n",
    "\n",
    "* **Activation** : 전기 신호 $\\approx$ 활성화 함수\n",
    "> 인간의 뇌의 시냅스 연결 - 무작위적, 비정형\n",
    ">\n",
    "> DNN의 퍼셉트론 간 연결 - 선형적?, 정형\n",
    "\n",
    "* **역전파 backpropagation**\n",
    "> 인간의 뇌 - 전체를 바꾸지 않고 일부만 변경하는 것이 가능\n",
    ">\n",
    "> DNN - 개별 weight만 바꿀 수 없음. 모든 뉴런을 다 변경해야 함 $\\to$ 비효율적, complexity 제약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045076f",
   "metadata": {},
   "source": [
    "`-` **Loss function**\n",
    "\n",
    "$$l(y, f({\\boldsymbol x}))$$\n",
    "\n",
    "* $l$은 일반적으로 convex function을 가정: CrossEntropy, MSE\n",
    "* $f(x)$가 정해져 있다면 loss function 자체만으로는 convex 하지만, $f$의 형태가 $\\theta$에 의해 바뀌기 때문에 loss function은 convex하지 않아 최적화가 어려움\n",
    "* 신경망의 앞부분은 Feature Extractor의 역할, 뒷부분은 손실 측정의 역할\n",
    "* $\\theta$의 비중이 압도적으로 큼. 즉, Feature extractor가 중요한 정보를 가지고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bf685",
   "metadata": {},
   "source": [
    "`-` 다양한 활용\n",
    "\n",
    "* **CNN**: `Convolution Kernel`을 도입하여 엣지 탐색, characterize로 이미지를 신경망이 더 잘 이해할 수 있도록 도움\n",
    "* **Shallow NN**: hidden layer가 두 개 이하인 경우 사용하기도 하는 표현\n",
    "* **Gradient vanishing**: Activation funciton 중 `sigmoid`,`tanh`는 각각 최대 편미분 값이 0.25, 1이기 때문에 레이어가 깊어질 수록 편미분 값들이 반복적으로 곱해지면서 1보다 작아지게 되고 기울기가 소실되도록 한다. 따라서 `ReLU`, `leaky ReLU`와 같이 적당한 범위 내에 값을 가질 수 있도록 하는 활성화 함수를 선호한다.\n",
    "* **Batch normalization**: 배치 단위로 입력을 정규화. 파라미터 스케일에 대한 민감도를 낮추는 역할\n",
    "* **Dropout**: 인간의 뇌가 랜덤하게 connection된다는 점을 모방하여 학습을 부드럽게 만듦\n",
    "* **Skip connection**: 레이어 연결 과정에서 몇 개의 층을 스킵하여 그래디언트 계산이 쉽도록 만듦\n",
    "* **Regularization**: 손실함수에 패널티를 추가하여 파라미터 크기를 너무 크지 않도록 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c86cb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "`-` **Stochastic Gradient Descent** : 랜덤 배치를 사용하는 경사 하강법\n",
    "\n",
    "* 전체 데이터 대신 무작위로 선택한 mini-batch를 사용하여 그래디언트를 계산, 작은 learning rate로 업데이트하는 과정을 반복\n",
    "* 일부의 데이터만 손실함수 계산에 사용함으로써 손실함수 개형을 바꿈\n",
    "* loss를 감소시키기는 어려우나, 특정 구간을 넘어가면 loss function의 flat 구간에 도착할 것으로 생각하고 최적화\n",
    "* 미니배치를 사용함에 따라 그래디언트에 노이즈를 발생시켜 bad local minimum에서 탈출시킬 수 있도록 도울 수 있음\n",
    "\n",
    "$$W_t \\leftarrow W_{t-1} - η \\nabla l_{W_{t-1}}(\\mathcal{B})$$\n",
    "\n",
    "* 이론적으로 learning_rate는 step $t$마다 일정한 규칙에 의해 줄어드는 것이 이상적임 $\\sum_t η_t = \\infty, ~ \\sum_t η_t^2 < \\infty \\to \\text{e.g.,} ~~ η_t = \\frac1t$\n",
    "* 실제로는 다양한 스케줄러가 사용되며, learning rate를 크게 만든 뒤 작게 조정하는 Cyclical learning rate는 NLP에서 사용하는 것이 좋다고 알려져 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3a4892",
   "metadata": {},
   "source": [
    "`-` **Data Loader**\n",
    "\n",
    "* 데이터의 용량이 매우 커서 GPU의 VRAM에 모두 올릴 수 없는 경우, 대용량의 데이터를 배치 단위로 나누어 순차적으로 GPU에 올려 학습\n",
    "* 배치를 사용하더라도 LLM 학습에서는 입력 시퀀스의 길이를 증가시켜 최대 출력 토큰 길이를 향상시키기 위해 VRAM이 더 많이 필요하게 됨\n",
    "* Data Loader는 CPU에서 미리 데이터를 준비한 뒤, GPU로 빠르게 전달하는 병렬 처리를 수행하여 효율적으로 배치를 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3f584",
   "metadata": {},
   "source": [
    "## 2. Transfer Learning (단순 전이 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d7bcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **A. Basic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ceef8",
   "metadata": {},
   "source": [
    "`-` pre-trained model and target task\n",
    "\n",
    "* pre-trained model이 주어졌을 때, 입력 데이터의 변환된 표현을 획득 가능\n",
    "* 해당 표현은 새로운 문제에서 source와 target이 공통 요소를 가질 때 유용\n",
    "> DNN에서 feature extractor 역할을 하는 부분은 다른 곳에 활용해도 잘 작동하지 않을까란 아이디어\n",
    ">\n",
    "> General한 pre-trained model을 다른 도메인에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf045a6",
   "metadata": {},
   "source": [
    "`-` Mathmetical Framework\n",
    "\n",
    "$T, S$가 Target, Source domain이라고 할 때,\n",
    "\n",
    "1. $g_i: X_s \\to Y_s$ hypothesis of source tasks. (함수의 형태가 이런 꼴일 것이라는 가설을 세우는 느낌으로 이해)\n",
    "2. $f: X_t \\to Y_s$ hypothesis of target task. 배우고 싶은 요소\n",
    "\n",
    "* common $c$, specific parts $w_i, v$로 hypothesis를 분해\n",
    "\n",
    "$$g_i = w_i \\circ c, ~ f = v \\circ c$$\n",
    "\n",
    "> $c$는 소스 데이터에서 학습 가능, $v$는 타겟 데이터로 학습 가능 $\\to$ $v$만 배우면 됨\n",
    ">\n",
    "> 대용량의 데이터셋으로 훈련된 모델 $g_i$와, $f$가 공유하는 특징이 클 수록 전이학습이 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48551562",
   "metadata": {},
   "source": [
    "`-` Scratch, Freezing, Fine-Tuning\n",
    "\n",
    "* 소스 모델의 마지막 output layer를 제거하고, target task를 위한 additional layers를 부착하여 end-to-end 구조로 구성됨\n",
    "* Scratch: 전체 가중치를 랜덤 초기값으로 설정하고, target 데이터로 전체 모델을 학습. 전체 모델을 재훈련함으로써 데이터가 적은 경우 모형이 제대로 학습되지 못함\n",
    "* Freezing: 소스 모델의 가중치는 동결한 채, additional layers만 훈련\n",
    "* Fine-Tuning: 소스 모델의 가중치를 초기값으로 설정하고, target 데이터로 전체 모델을 학습\n",
    "* Hybrid: Feature Extractor(소스 모델)의 일부 레이어를 동결시키고, 일부 레이어만 기존 가중치를 초기값으로 하여 additional layers와 같이 훈련. 어떤 지점에서 구간을 잘라야 할 지 결정하는 문제가 있으므로, 일반적으로 활용되지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdcf06",
   "metadata": {},
   "source": [
    "### **B. Mathmatical Frameworks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9344f6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37786e0e",
   "metadata": {},
   "source": [
    "## 3. Domain Adaptation / Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad823c7c",
   "metadata": {},
   "source": [
    "## 4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e9d93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
