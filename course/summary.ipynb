{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a381f223",
   "metadata": {},
   "source": [
    "## 1. AI에 대한 개괄적 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41668ec1",
   "metadata": {},
   "source": [
    "### **A. 인공지능**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ab4b6",
   "metadata": {},
   "source": [
    "명확하게 정리된 유일한 정의는 없음\n",
    "\n",
    "`-` **AI에 대한 기술적 정의 (Technical definition)**:\n",
    "* 인간의 오감과 관련된 정보를 통합하여 분석, 학습, 예측\n",
    "* 의사결정 수행\n",
    "* 새로운 정보 학습 시 이전에 배운 정보를 유지하여 학습 가능: Continual Learning $\\to$ 새로운 태스크 데이터 학습 시 기존의 특정 태스크 정보가 점점 잊혀지는데, 이를 해결\n",
    "* 언어 처리와 메타 러닝 등을 수행할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a73407",
   "metadata": {},
   "source": [
    "`-` 뇌과학 관점에서의 인공지능\n",
    "\n",
    "* Neural Network\n",
    "* 인간의 뇌를 모방"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434d988",
   "metadata": {},
   "source": [
    "`-` 공학적 관점에서의 인공지능\n",
    "\n",
    "* XOR 문제의 해결을 위한 Multi-Layer-Perceptron\n",
    "* 통계학에서의 비선형 모델링 $y = f(x) + \\epsilon$ -> $f$의 설계\n",
    "* MLP $f = g_2 \\circ g_1$ : 미분 어려움, 해석에서 문제 있음\n",
    "> Complexity 훨씬 높음, 다양한 문제에 적용 가능 $\\to$ DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a16d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "`-` Application Example\n",
    "\n",
    "* Precision Medicine: 의료의 개인화 Personalization\n",
    "* Virtual Assistant: 가상 비서\n",
    "* Expert System\n",
    "* Auto Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049e277",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "* 추론 Reasoning\n",
    "* 자율적인 의사결정 및 작업 수행 Agent\n",
    "* Physical AI\n",
    "* Computer Vision/Speech/Expert System은 이미 인간을 능가하였음\n",
    "* Continuous learning/Meta learning/Reinforcement learning에서 상당한 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b423a56",
   "metadata": {},
   "source": [
    "### **B. DNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91534ac8",
   "metadata": {},
   "source": [
    "`-` Simple DNN (Multi-Layer Perceptron; MLP)\n",
    "\n",
    "$$f({\\boldsymbol x}) = a(g_L \\circ g_{L-1} \\circ \\cdots \\circ g_1 ({\\boldsymbol x}))$$\n",
    "\n",
    "* $g_l(\\boldsymbol x) = \\sigma ({\\boldsymbol W_l \\boldsymbol x +  \\boldsymbol b_l})$이며, 해당 함수 $f$는 레이블 예측에 활용됨. 파라미터를 통해 학습되는 함수.\n",
    "* $\\sigma$는 활성화 함수(activation function)이며, $a(\\cdot) = {\\boldsymbol W_a ~ \\cdot} + {\\boldsymbol b_a}$는 스칼라 함수 또는 벡터 함수임(netout 벡터를 반환)\n",
    "\n",
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "\n",
    "> 단순 레이어 변환. 선형 변환임: $\\boldsymbol W_l  \\boldsymbol x$의 형태\n",
    "\n",
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\overset{\\sigma}{\\to} \\begin{bmatrix} \\sigma (h_1) \\\\ \\sigma (h_2) \\end{bmatrix}$$\n",
    "\n",
    "> 해당 선형 변환에 ReLU, sigmoid 등의 비선형 변환(activation function)을 추가하여 복잡한 형태를 만들어냄. $W$의 차원은 자유롭게 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ea660",
   "metadata": {},
   "source": [
    "`-` 인간의 뉴런과 DNN의 차이\n",
    "\n",
    "* **Activation** : 전기 신호 $\\approx$ 활성화 함수\n",
    "> 인간의 뇌의 시냅스 연결 - 무작위적, 비정형\n",
    ">\n",
    "> DNN의 퍼셉트론 간 연결 - 선형적?, 정형\n",
    "\n",
    "* **역전파 backpropagation**\n",
    "> 인간의 뇌 - 전체를 바꾸지 않고 일부만 변경하는 것이 가능\n",
    ">\n",
    "> DNN - 개별 weight만 바꿀 수 없음. 모든 뉴런을 다 변경해야 함 $\\to$ 비효율적, complexity 제약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045076f",
   "metadata": {},
   "source": [
    "`-` **Loss function**\n",
    "\n",
    "$$l(y, f({\\boldsymbol x}))$$\n",
    "\n",
    "* $l$은 일반적으로 convex function을 가정: CrossEntropy, MSE\n",
    "* $f(x)$가 정해져 있다면 loss function 자체만으로는 convex 하지만, $f$의 형태가 $\\theta$에 의해 바뀌기 때문에 loss function은 convex하지 않아 최적화가 어려움\n",
    "* 신경망의 앞부분은 Feature Extractor의 역할, 뒷부분은 손실 측정의 역할\n",
    "* $\\theta$의 비중이 압도적으로 큼. 즉, Feature extractor가 중요한 정보를 가지고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bf685",
   "metadata": {},
   "source": [
    "`-` 다양한 활용\n",
    "\n",
    "* **CNN**: `Convolution Kernel`을 도입하여 엣지 탐색, characterize로 이미지를 신경망이 더 잘 이해할 수 있도록 도움\n",
    "* **Shallow NN**: hidden layer가 두 개 이하인 경우 사용하기도 하는 표현\n",
    "* **Gradient vanishing**: Activation funciton 중 `sigmoid`,`tanh`는 각각 최대 편미분 값이 0.25, 1이기 때문에 레이어가 깊어질 수록 편미분 값들이 반복적으로 곱해지면서 1보다 작아지게 되고 기울기가 소실되도록 한다. 따라서 `ReLU`, `leaky ReLU`와 같이 적당한 범위 내에 값을 가질 수 있도록 하는 활성화 함수를 선호한다.\n",
    "* **Batch normalization**: 배치 단위로 입력을 정규화. 파라미터 스케일에 대한 민감도를 낮추는 역할\n",
    "* **Dropout**: 인간의 뇌가 랜덤하게 connection된다는 점을 모방하여 학습을 부드럽게 만듦\n",
    "* **Skip connection**: 레이어 연결 과정에서 몇 개의 층을 스킵하여 그래디언트 계산이 쉽도록 만듦\n",
    "* **Regularization**: 손실함수에 패널티를 추가하여 파라미터 크기를 너무 크지 않도록 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c86cb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "`-` **Stochastic Gradient Descent** : 랜덤 배치를 사용하는 경사 하강법\n",
    "\n",
    "* 전체 데이터 대신 무작위로 선택한 mini-batch를 사용하여 그래디언트를 계산, 작은 learning rate로 업데이트하는 과정을 반복\n",
    "* 일부의 데이터만 손실함수 계산에 사용함으로써 손실함수 개형을 바꿈\n",
    "* loss를 감소시키기는 어려우나, 특정 구간을 넘어가면 loss function의 flat 구간에 도착할 것으로 생각하고 최적화\n",
    "* 미니배치를 사용함에 따라 그래디언트에 노이즈를 발생시켜 bad local minimum에서 탈출시킬 수 있도록 도울 수 있음\n",
    "\n",
    "$$W_t \\leftarrow W_{t-1} - η \\nabla l_{W_{t-1}}(\\mathcal{B})$$\n",
    "\n",
    "* 이론적으로 learning_rate는 step $t$마다 일정한 규칙에 의해 줄어드는 것이 이상적임 $\\sum_t η_t = \\infty, ~ \\sum_t η_t^2 < \\infty \\to \\text{e.g.,} ~~ η_t = \\frac1t$\n",
    "* 실제로는 다양한 스케줄러가 사용되며, learning rate를 크게 만든 뒤 작게 조정하는 Cyclical learning rate는 NLP에서 사용하는 것이 좋다고 알려져 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3a4892",
   "metadata": {},
   "source": [
    "`-` **Data Loader**\n",
    "\n",
    "* 데이터의 용량이 매우 커서 GPU의 VRAM에 모두 올릴 수 없는 경우, 대용량의 데이터를 배치 단위로 나누어 순차적으로 GPU에 올려 학습\n",
    "* 배치를 사용하더라도 LLM 학습에서는 입력 시퀀스의 길이를 증가시켜 최대 출력 토큰 길이를 향상시키기 위해 VRAM이 더 많이 필요하게 됨\n",
    "* Data Loader는 CPU에서 미리 데이터를 준비한 뒤, GPU로 빠르게 전달하는 병렬 처리를 수행하여 효율적으로 배치를 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3f584",
   "metadata": {},
   "source": [
    "## 2. Transfer Learning (단순 전이 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d7bcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **A. Basic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ceef8",
   "metadata": {},
   "source": [
    "`-` pre-trained model and target task\n",
    "\n",
    "* pre-trained model이 주어졌을 때, 입력 데이터의 변환된 표현을 획득 가능\n",
    "* 해당 표현은 새로운 문제에서 source와 target이 공통 요소를 가질 때 유용\n",
    "> DNN에서 feature extractor 역할을 하는 부분은 다른 곳에 활용해도 잘 작동하지 않을까란 아이디어\n",
    ">\n",
    "> General한 pre-trained model을 다른 도메인에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf045a6",
   "metadata": {},
   "source": [
    "`-` Mathmetical Framework\n",
    "\n",
    "$T, S$가 Target, Source domain이라고 할 때,\n",
    "\n",
    "1. $g_i: X_s \\to Y_s$ hypothesis of source tasks. (함수의 형태가 이런 꼴일 것이라는 가설을 세우는 느낌으로 이해)\n",
    "2. $f: X_t \\to Y_t$ hypothesis of target task. 배우고 싶은 요소\n",
    "\n",
    "* common $c$, specific parts $w_i, v$로 hypothesis를 분해\n",
    "\n",
    "$$g_i = w_i \\circ c, ~ f = v \\circ c$$\n",
    "\n",
    "> $c$는 소스 데이터에서 학습 가능, $v$는 타겟 데이터로 학습 가능 $\\to$ $v$만 배우면 됨\n",
    ">\n",
    "> 대용량의 데이터셋으로 훈련된 모델 $g_i$와, $f$가 공유하는 특징이 클 수록 전이학습이 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48551562",
   "metadata": {},
   "source": [
    "`-` Scratch, Freezing, Fine-Tuning\n",
    "\n",
    "* 소스 모델의 마지막 output layer를 제거하고, target task를 위한 additional layers를 부착하여 end-to-end 구조로 구성됨\n",
    "* Scratch: 전체 가중치를 랜덤 초기값으로 설정하고, target 데이터로 전체 모델을 학습. 전체 모델을 재훈련함으로써 데이터가 적은 경우 모형이 제대로 학습되지 못함\n",
    "* Freezing: 소스 모델의 가중치는 동결한 채, additional layers만 훈련\n",
    "* Fine-Tuning: 소스 모델의 가중치를 초기값으로 설정하고, target 데이터로 전체 모델을 학습\n",
    "* Hybrid: Feature Extractor(소스 모델)의 일부 레이어를 동결시키고, 일부 레이어만 기존 가중치를 초기값으로 하여 additional layers와 같이 훈련. 어떤 지점에서 구간을 잘라야 할 지 결정하는 문제가 있으므로, 일반적으로 활용되지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdcf06",
   "metadata": {},
   "source": [
    "### **B. Mathmatical Frameworks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f98346",
   "metadata": {},
   "source": [
    "`-` Risk of Transfer Learning\n",
    "\n",
    "$C(\\mathcal{C}), C(\\mathcal{V})$를 $\\mathcal{C}, \\mathcal{V}$의 복잡도 측정값이라 하고, $t, n$을 각각 소스 태스크의 수와 개별 소스 태스크에서의 샘플 사이즈로, $m$을 타겟 태스크의 샘플 사이즈라고 할 때\n",
    "\n",
    "$$\\tilde{O} \\left( \\frac{1}{\\nu} \\sqrt{\\frac{C(\\mathcal{C}) + t C(\\mathcal{V})}{nt}} + \\sqrt{\\frac{C(\\mathcal{V})}{m}} \\right)$$\n",
    "\n",
    "* 현재 target sample size인 $m$을 키우기 어려운 상황이므로, 완전히 다른 태스크인 $\\mathcal{V}$의 비중이 줄어들어야 한다. 즉, 소스와 타겟의 공통 태스크 비중이 커야 복잡도가 줄어든다.\n",
    "* 또한 소스 모델이 큰 모델일 수록 $nt$의 값이 커져 복잡도가 감소한다.\n",
    "* 즉, 좋은 소스 모델을 찾아서 타겟 문제를 해결하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8652c6b",
   "metadata": {},
   "source": [
    "`-` Transfer Learning과 관련된 문제\n",
    "\n",
    "* Meta Learning: 학습을 위한 학습. 여러 언어들을 배운 모델이 다른 언어를 쉽게 학습할 수 있을까?\n",
    "* Adaptive Learning: 배운 지식을 공통점이 조금 있는 다른 분야에서 활용할 수 있을까?\n",
    "* OOD: 학습 데이터의 분포와 평가 데이터의 분포가 다른 상황에서 모델이 정상 작동할 수 있을까? 현실에서의 문제 해결에 매우 중요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37786e0e",
   "metadata": {},
   "source": [
    "## 3. Transfer Learning (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e57aa",
   "metadata": {},
   "source": [
    "### **A. Domain Adaptation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0d2e4",
   "metadata": {},
   "source": [
    "`-` Definition\n",
    "\n",
    "* 소스 도메인의 데이터로 훈련된 모델을 관련된 다른 타겟 도메인에서 잘 작동하도록 만드는 기술\n",
    "\n",
    "> Superviesd to Supervised\n",
    ">\n",
    "> 적은 라벨의 지도학습을 많은 라벨의 지도학습으로\n",
    ">\n",
    "> Unsupervised to Unsupervised: PCA 변환 행렬을 다른 데이터에 그대로 사용할 수 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461c3a5",
   "metadata": {},
   "source": [
    "`-` Examples\n",
    "\n",
    "* 특정 지역의 저해상도의 이미지와 레이블 + 레이블링 되지 않은 고해상도 이미지\n",
    "* 다수의 레이블로 처리된 시뮬레이션 이미지 + 더 적은 레이블의 현재 이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3abb5a2",
   "metadata": {},
   "source": [
    "`-` Mathematical formulation\n",
    "\n",
    "$$\\begin{align} & p_t(x) \\neq p_s(x) \\\\\n",
    "& p_t(T(x)) = p_s(T(x)) \\\\\n",
    "& p_t(y | T(x)) = p_s(y | T(x))\n",
    "\\end{align}$$\n",
    "\n",
    "$p_t,~ p_s$가 타겟/소스 도메인의 분포이고, $T$를 변환이라고 할 때, 타겟과 소스의 분포를 일치시키는 변환 $T$를 찾는다.\n",
    "\n",
    "* 서로 다른 분포의 소스/타겟 도메인이 존재할 때, $T$라는 변환은 각 도메인의 데이터 x에 대하여 $T(x)$가 같은 분포를 가지도록 만든다.\n",
    "* 해당 변환을 찾을 수 있다면, 각 도메인에서 동일한 변환값이 주어졌을 때 label의 조건부 확률 분포는 동일해야 한다 $\\to$ 변환하여 분류할 수 있는 형태로 주어진다.\n",
    "\n",
    "> 현실에서 마지막은 이상적인 경우이며, $T$를 찾는 문제도 매우 어렵다. 보통 $T$를 찾고, 마지막 조건에 근접하도록 regularization 한다.\n",
    ">\n",
    "> 두 데이터가 하나의 확률 분포를 가지고 있으면 동일한 예측을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bd627",
   "metadata": {},
   "source": [
    "`-` **Metric Learning**\n",
    "\n",
    "1. 두 점 $x_s^i,~ x_t^i$ 사이의 거리를 고려\n",
    "\n",
    "$$d_W (x_s^i, x_t^i) = (x_s^i - x_t^i)^{\\top} W (x_s^i - x_t^i)$$\n",
    "\n",
    "> 유클리디안 거리의 제곱 수식에 positive semi-definite metrix $W$를 넣음.\n",
    "\n",
    "2. $W$는 $W^{\\frac12}$로 나타날 수 있으므로, 위 수식은 $W^{\\frac12}x_s^i$와 $W^{\\frac12}x_t^i$간 거리: 각 점을 변환한 상태에서의 거리라고 말할 수 있음. (선형 변환)\n",
    "\n",
    "3. $x_s^i,~ x_t^i$의 레이블이 같다면, 변환 후 거리가 $u$를 넘기지 않아야 하며, 레이블이 다르다면 변환 후 거리가 $l$보다 길어야 함.\n",
    "\n",
    "$$\\text{arg} \\min_{W ⪰ 0} Tr(W) - \\log \\text{det} (W) ~~ s.t.$$\n",
    "\n",
    "$$d_W (x_s^i, x_t^i) ≤ u ~~~~ \\text{if} ~~ y^i = y^j$$\n",
    "\n",
    "$$d_W (x_s^i, x_t^i) ≥ l ~~~~ \\text{if} ~~ y^i \\neq y^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f5616",
   "metadata": {},
   "source": [
    "`-` **Asymmetric Transformations**\n",
    "\n",
    "1. 내적 기반 유사도를 사용 (Inner product-based similarity)\n",
    "\n",
    "$$\\text{sim}_W (x_s^i, x_t^i) = {x_s^i}^{\\top} W x_t^i$$\n",
    "\n",
    "> 내적의 값이 클수록 유사하지 않은 것\n",
    "\n",
    "2. Loss function\n",
    "\n",
    "$$\\mathbb{I}(y_s^i = y_t^i) (\\max(0, l - {x_s^t}^{\\top} W x_t^i))^2 + \\mathbb{I}(y_s^i \\neq y_t^i) (\\max(0, {x_s^i}^{\\top} W x_y^i - u))^2 + \\lambda ||W||_F$$\n",
    "\n",
    "* 0보다 작을 수 없는 Loss function\n",
    "* 같은 레이블일 경우 l보다 큰 유사도를, 다른 레이블일 경우 u보다 작은 유사도를 가져야 손실을 작게 유지 가능\n",
    "* $W$의 값 자체가 너무 커지는 것을 방지하기 위해 Frobenius norm을 사용: 모든 원소의 제곱합의 제곱근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cac1e8",
   "metadata": {},
   "source": [
    "위의 두 방법은 모두 하나의 행렬을 기반으로 한 선형 방법이므로, 비선형 변환이 필요한 문제에서 잘 동작하지 못할 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d57da1",
   "metadata": {},
   "source": [
    "`-` **Maximum Mean Discrepancy**: Unsupervised Domain Adaptation. 두 분포 간 거리를 측정하는 metric\n",
    "\n",
    "* 두 분포가 같은지를 판단하기 위한 가장 단순한 방법: Moment Matching\n",
    "> 두 도메인 $X, Y$의 $1, 2, \\cdots, m$차 적률이 동일한지를 검사한다. 이론적으로 모든 적률이 존재하고 수렴한다는 조건 하에서 $m \\to \\infty$일 때 이것이 성립된다면 $p_s = p_t$라 말할 수 있다.\n",
    "\n",
    "* MMD의 아이디어: RKHS인 $H$ 상에서 $\\text{MMD}(X, Y) = \\underset{f \\in H, ~ ||f|| ≤ 1}{\\sup} ~ E[f(X)] - E[f(Y)] \\to 0$이면, $X, Y$의 분포는 동일하다. $\\to$ $H$에서의 변환 후 적률 차이의 상한이 0이면 둘은 동일...\n",
    "\n",
    "* Sample MMD: Kernel Trick (Inner Product ↔ Kernel. 고차원 매핑의 내적은 커널로 표현할 수 있음)\n",
    "\n",
    "$$⟨\\phi(x),\\phi(y)⟩ = K(x,y)$$\n",
    "\n",
    "$$\\text{MMD}(\\{x_i\\}_{i=1}^n, ~ \\{y_i \\}_{i=1}^m)^2 = \\sum_{i, j} K(x_i, x_j)/n^2 + \\sum_{i, j} K(y_i, y_j)/m^2 - 2 \\sum_{i, j} K(x_i, y_i)/mn$$\n",
    "\n",
    "> 변환된 소스-타겟 샘플에 대하여 평균적으로 $K(x_i, x_j) = K(x_i, y_j) = K(y_i, y_j)$가 된다면 sample MMD는 0에 가까워진다.\n",
    ">\n",
    "> Exponential 형태의 커널 $K(x, y) = \\exp(-\\frac{(x-y)^2}{\\sigma})$등을 사용하면 충분한 표현력을 가지며, 분포의 동질성을 식별할 수 있다. 즉, sample MMD(X, Y)를 최소화하는 학습을 통해 두 분포를 같게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a74ea0",
   "metadata": {},
   "source": [
    "`-` **Sample Reweighting**: Unsupervised DA\n",
    "\n",
    "$$\\text{arg} \\min_{\\beta} || \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\phi (x_s^i) - \\frac{1}{m} \\sum_{i=1}^m \\phi (x_t^i)||^2$$\n",
    "\n",
    "$$s.t. ~~ \\beta_i \\in [0, B], ~ i \\in [n], ~ |\\sum_{i=1}^n \\beta_i - n| ≤ n\\epsilon$$\n",
    "\n",
    "* 파라미터 $\\beta_i$가 추가된 MMD 손실 함수로 분포를 같게 만듦\n",
    "* $\\beta_i$를 곱하여 소스 샘플별 가중치를 조정함으로써 두 도메인 데이터의 분포를 맞춤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b837b",
   "metadata": {},
   "source": [
    "`-` **Sample Selection**: Sample Reweighting의 확장\n",
    "\n",
    "$$\\text{arg} \\min_{\\alpha} || \\frac{1}{\\sum_i \\alpha_i} \\sum_{i=1}^n \\alpha_i \\phi (x_s^i) - \\frac{1}{m} \\sum_{i=1}^m \\phi (x_t^i)||^2$$\n",
    "\n",
    "$$s.t. ~~ \\alpha_i \\in \\{0, 1\\}, ~ i \\in [n], ~ \\sum_i \\frac{1}{\\sum_i \\alpha_i} \\alpha_i y_c^i = \\frac{1}{n} \\sum_i y_c^i$$\n",
    "\n",
    "* 단순 연속형 가중치 $\\beta_i$를 주는 대신, 선택 변수 $\\alpha_i \\in \\{0, 1\\}$을 부여하여 유사한 샘플만 선택\n",
    "* 손실 계산에 사용할 샘플 선택 후 레이블의 비율이 원래와 같게 유지되도록 제한하여 학습 시 클래스 비율 변경에 의해 모델이 왜곡되지 않도록 함\n",
    "> 타겟 분포와 유사하면서, 레이블 비율도 유지하는 소스 샘플만 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c942dd9",
   "metadata": {},
   "source": [
    "`-` **Domain Invariant Projection (DIP)**\n",
    "\n",
    "$$\\text{arg} \\min_{W} D_{\\text{MMD}}^2 (W^{\\top} X_s, W^{\\top} X_t)$$\n",
    "\n",
    "$$s.t ~~ W^{\\top}W = I$$\n",
    "\n",
    "* 정직교 orthogonal 행렬인 $W$로 두 도메인을 선형 변환한 뒤, sample MMD를 계산하고 이를 최소화하는 $W$를 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99cbf6",
   "metadata": {},
   "source": [
    "`-` **Domain Invariant Projection (DIP)**\n",
    "\n",
    "* 선형 사상인 $W$를 이용하여 두 도메인을 같은 공간으로 사영한 뒤, 사영된 공간에서 MMD를 다시 최소화\n",
    "* 선형 변환된 것을 한번 더 MMD로 재변환. $W_x \\to \\phi(W_x)$\n",
    "* 모형 자체가 비선형인 경우(DNN 등)가 들어오면, 선형 변환이 깨짐. 따라서 비선형으로 변환하여 성능을 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dfa44",
   "metadata": {},
   "source": [
    "`-` **Correlation Approach CORAL**\n",
    "\n",
    "* 소스와 타겟의 correlation을 맞춤\n",
    "* 소스 피쳐를 de-correlate 이후, 타겟 피쳐의 공분산을 이용해 re-correlate\n",
    "* 도메인의 분포가 정규분포를 벗어나면 해당 접근이 잘 동작하지 않을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516eea3",
   "metadata": {},
   "source": [
    "### **B. Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1854ba",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580275a",
   "metadata": {},
   "source": [
    "* 학생과 선생 구조\n",
    "* 학생이 선생을 따라가게 만드는 구조로 설계하면 어떨까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c5e90",
   "metadata": {},
   "source": [
    "`-` ReLU activation function\n",
    "\n",
    "* activation이 0일 경우, 정보가 넘어가지 않음 $\\to$ 그 뒤로도 연결이 계속 끊어짐. 불필요한 파라미터가 많음\n",
    "* 뉴런 간 connection이 대다수 유지되지 못함. 많이 끊어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b49ff",
   "metadata": {},
   "source": [
    "`-` 모델 아키텍쳐의 요약\n",
    "\n",
    "* 고성능의 대형 모델(Teacher)과 비슷한 결과가 나오는 작은 모델(Student)을 만들 수 있지 않을까?\n",
    "* 전체 파라미터 구조와 학습하는 데이터의 규모가 작음. Teacher의 모든 지식을 전수받을 순 없으나, 꽤나 높은 성능을 발휘함\n",
    "\n",
    "> 학습에 있어 어느 부분이 핵심 정보를 가지고 있는가? 에 대한 파악이 필요.\n",
    ">\n",
    "> 어떤 레이어가 가장 많은 정보? 층 자체가 student와 유사한 패턴을? relation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abe252",
   "metadata": {},
   "source": [
    "`-` Knowleage Distillation\n",
    "\n",
    "* 기존 고성능 모델보다 간단한 아키텍쳐로 조금 성능이 떨어지는 고효율의 모델을 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de281794",
   "metadata": {},
   "source": [
    "### **Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749df852",
   "metadata": {},
   "source": [
    "`-` teacher와 student가 산출하는 확률 벡터를 가능하면 동일하게 만들도록 함: Distillation Loss\n",
    "\n",
    "* Response-based: 타겟 기반. $y$에 대한 예측의 쌍이 맞도록(확률 벡터가 유사하도록)\n",
    "* Feature-based: First Layer vector $\\to$ Input, Attribute. 중간 레이어의 변환값 $\\to$ Feature. 확률 벡터 산출 이전의 feature layer 값이 유사하도록 (보통 바로 전 레이어 netout)\n",
    "* Relation-based: 위 두 방식은 정보가 말단 레이어에 몰려있을 것이라고 생각함 $\\to$ Feature가 다음 레이어로 넘어가는 과정이 유사하도록 - 레이어 간 correlation을 유사하게 만듦\n",
    "\n",
    "> sota는 없음. 상황에 따라 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b65573",
   "metadata": {},
   "source": [
    "`-` Response-based approaches (확률값이므로 유사성 판별을 따로 고려)\n",
    "\n",
    "* KL divergence: teacher와 student 간 확률이 동일하도록 만드는 loss function을 택함. 기존 손실에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bac0f",
   "metadata": {},
   "source": [
    "`-` Feature-based\n",
    "\n",
    "* 단순히 feature vector 간 거리로 판단 가능. 하지만 두 벡터 간 스케일이 다르기 때문에 비교 이전에 norm으로 나누어 스케일을 조정한 뒤 비교해야 함\n",
    "* 상수와 norm으로 스케일 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7004a",
   "metadata": {},
   "source": [
    "`-` Relaetd-based\n",
    "\n",
    "* 2개 레이어 간 inner product가 유사한 정도 (스케일 조정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9b1a6",
   "metadata": {},
   "source": [
    "`-` Other categories\n",
    "\n",
    "* Online/Offline: Online Learning $\\to$ 새로 들어온 데이터를 추가했을 때 어떻게 바꿔야 수학적으로 엄밀한지를 분석하여 조정. 새로 들어온 데이터 단독으로 모델을 구성하여 가중치를 변경\n",
    "> convex에서는 잘 작동함. non-convex 문제에서는 이론적인 background는 없음. convergence가 얼마나 잘 되는지 정도에 국한함.\n",
    "* self-distillation: 자기 자신이 teacher이자 student\n",
    "* multi-teacher: 여러 teacher를 사용\n",
    "* adversatial approach: 적대적 모형을 동시에 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad823c7c",
   "metadata": {},
   "source": [
    "## 4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e9d93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
