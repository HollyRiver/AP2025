{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd24dd1",
   "metadata": {},
   "source": [
    "* Transfer Learning\n",
    "> 코딩을 해라 이런건 안함\n",
    ">\n",
    "> 개념, 전이학습이 뭔지\n",
    "\n",
    "* Bayesian 기본 개념\n",
    "* Prior 주고 Posterior 유도하는 문제 같은 건 안줄거임. 계산하는 거. normal-normal 이런건?\n",
    "> 결과를 어떻게 해석하는지, 그 정도의 영역.\n",
    "\n",
    "* Conjugate, Exponential Family가 뭔지를 정확히\n",
    "> Conjugate: Prior와 Likelihood 쌍으로 정의. Prior와 Likelihood로 이뤄진 Posterior가 Prior와 동일한 family에 속하면 Conjugate\n",
    "\n",
    "* Sampling을 왜 하는가? Reject, Importance, Random 어떻게 하는지\n",
    "* Markov chain 정의, Stationary distribution을 만족하기 위한 세 가지 조건: 어떻게 얻는지.\n",
    "> 전이행렬 하나만 있으면 stationary distribution을 얻을 수 있음. $\\pi^{\\top}P = \\pi^{\\top}$을 만족하는 애를 찾으면 됨.\n",
    ">\n",
    "> 세 가지 조건을 만족하면 stationary distribution이 유일하게 나옴. Transition probability가 모든 것을 결정함\n",
    ">\n",
    "> 정상분포를 얻기 위해서 어떠한 P가 필요한가? P를 잘 만들자: MH 알고리즘.\n",
    "\n",
    "* Gelman test: 초기값을 바꾸어 여러 개의 샘플링을 수행하고 비슷하게 수렴하는지를 확인\n",
    "\n",
    "* MH algorithm\n",
    "> $(1-\\alpha)\\delta_x (y) + \\alpha k(x, y)$\n",
    ">\n",
    "> 지금보다 확률이 낮은 경우에는 잘 안감. 시간이 오래 걸리고 같은 값을 계속 샘플링하는 경우가 필연적으로 생긴다는 한계.\n",
    "\n",
    "* Naive Bayes\n",
    "> $p(\\theta_1, \\theta_2) \\approx \\prod_{i=1}^2 q_{\\phi}(\\theta)$\n",
    ">\n",
    "> Variational Bayes: 독립 조건 하에서 ELBO를 최대화 -> $\\log p(y) - \\text{ELBO} = \\text{KL}(q_{\\phi}(\\theta)~||~p(\\theta~|~y))$를 최소화.\n",
    ">\n",
    "> $\\text{ELBO} = E_q[\\log p(y~|~\\theta)] - \\text{KL}(q_{\\phi}(\\theta)~||~p(\\theta))$\n",
    ">\n",
    "> ELBO 유도 -> maximization -> minimization : 중요함. 낼거임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5044235",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
