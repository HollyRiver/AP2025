{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18f7c79",
   "metadata": {},
   "source": [
    "# AI에 대한 개괄적인 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de6503",
   "metadata": {},
   "source": [
    "## 1. 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b14e",
   "metadata": {},
   "source": [
    "`-` 뇌과학 관점\n",
    "\n",
    "* Neural Network\n",
    "* 인간의 뇌를 모방"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff470c",
   "metadata": {},
   "source": [
    "`-` 공학적 관점\n",
    "\n",
    "* XOR 문제 해결 : Linear-Perceptron으로는 해결 불가\n",
    "> Non-Linear 형태로 풀이하여 해결 - Multi-Layer-Perceptron\n",
    "* 통계학에서의 비선형 모델링 $y = f(x) + \\epsilon$ -> $f$의 설계\n",
    "* MLP $f = g_2 \\circ g_1$ : 미분 어려움, 해석에서 문제 있음\n",
    "> Complexity 훨씬 높음, 다양한 문제에 적용 가능 $\\to$ DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de310731",
   "metadata": {},
   "source": [
    "## 2. DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e11975",
   "metadata": {},
   "source": [
    "`-` MLP 정의$(2 \\times 2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76dd99c",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a530a2f",
   "metadata": {},
   "source": [
    "> 단순 레이어 변환. 선형 변환임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bf162",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\overset{\\sigma}{\\to} \\begin{bmatrix} \\sigma (h_1) \\\\ \\sigma (h_2) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698eecc9",
   "metadata": {},
   "source": [
    "> ReLU, sigmoid 등의 비선형 변환을 추가하여 복잡한 형태를 만들어냄. $W$의 차원은 자유롭게 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76be44f",
   "metadata": {},
   "source": [
    "$$f({\\boldsymbol x}) = a(g_L \\circ g_{L-1} \\circ \\cdots \\circ g_1({\\boldsymbol x}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb232776",
   "metadata": {},
   "source": [
    "`-` Loss Function $l(y,f(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac72b9",
   "metadata": {},
   "source": [
    "> 일반적으로 Convex Function을 가정\n",
    ">\n",
    "> softmax loss(CrossEntropy), MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da5cfb",
   "metadata": {},
   "source": [
    "* $f(x)$의 형태가 이미 정해져 있다면, loss function은 convex함. 하지만 $f$의 형태를 바꾸는 $\\theta : x \\to f$도 파악해야 함. convex하지 않아 최적화가 어려움\n",
    "* convex가 되면 미분해서 0이 되는 지점만 구하면 됨, non-convex하다면 미분해서 0이 되는 포인트가 많아짐\n",
    "* $\\theta$의 비중이 압도적으로 큼. Feature extractor가 중요한 정보를 가지고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92588663",
   "metadata": {},
   "source": [
    "`-` 인간의 뉴런과 DNN의 차이\n",
    "\n",
    "* Activation : 전기 신호 $\\approx$ 활성화 함수\n",
    "> 인간의 뇌의 시냅스 연결 - 무작위적, 비정형\n",
    ">\n",
    "> DNN의 퍼셉트론 간 연결 - 선형적?, 정형\n",
    "\n",
    "* 역전파\n",
    "> 인간의 뇌 - 전체를 바꾸지 않고 일부만 변경하는 것이 가능\n",
    ">\n",
    "> DNN - 개별 weight만 바꿀 수 없음. 모든 뉴런을 다 변경해야 함 $\\to$ 비효율적, complexity 제약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987a115",
   "metadata": {},
   "source": [
    "`-` CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c394b",
   "metadata": {},
   "source": [
    "* 전체 이미지에서 엣지를 찾지 않고, 개별 커널에서 엣지 탐색 수 convolution으로 통합 $\\to$ 반복을 통해 엣지 정보를 추출 : characterize\n",
    "> 이미지 처리, 인식에 대해서 뉴럴 네트워크가 더 잘 이해할 수 있도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193ff2e",
   "metadata": {},
   "source": [
    "`-` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9541a",
   "metadata": {},
   "source": [
    "* Shallow NN : 히든 레이어를 적게\n",
    "\n",
    "$$\\text{input layer} \\to h_1 \\to h_2 \\to \\text{output layer}$$\n",
    "\n",
    "* Deep NN : 히든 레이어가 3개 이상... 다 말이 다르네\n",
    "\n",
    "$$\\text{input layer} \\to h_1 \\to h_2 \\to h_3 \\to \\cdots \\to \\text{output layer}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ab33c",
   "metadata": {},
   "source": [
    "* 미분값 소실 : activate function $\\sigma'(\\cdot)$의 미분값 비율만큼 레이어마다 그래디언트가 감소 $\\to$ vanishing $\\to$ 적당한 범위 내에 값을 가질 수 있도록 하는 활성화 함수를 선호\n",
    "> sigmoid, tanh $\\to$ 범위를 한정하기 때문에 vanishing 발생 가능 $\\to$ 미분을 하면 (0, 1)의 값을 가지게 됨...?\n",
    ">\n",
    "> ReLU로 풀면 그래디언트 잘 안사라짐 ㅇㅇ $\\to$ 미분을 해도 0아니면 1이 되니까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd0e9",
   "metadata": {},
   "source": [
    "* batch normalization : 계수가 너무 크면 곱해지면서 폭발 $\\to$ 정규화 필요\n",
    "\n",
    "* dropout : 인간의 뇌가 랜덤하게 connection되는 것을 모방했다고 말할 수 있음 $\\to$ 학습이 부드러워짐\n",
    "\n",
    "* regularization : 모수 값을 너무 크지 않게, l1/l2 norm 등 사용\n",
    "\n",
    "* skip connection : 레이어 연결 과정에서 스킵하여 그래디언트 계산이 쉽도록 함\n",
    "> loss function의 형태가 convex와 상당히 유사해짐... 이게 좋은건가, 어케함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9ed04",
   "metadata": {},
   "source": [
    "`-` Stochastic Gradient Descent : 랜덤 배치를 사용하는 경사 하강법\n",
    "\n",
    "> 일부의 데이터만 손실함수 계산에 사용함으로써 손실함수 개형을 바꿈\n",
    ">\n",
    "> loss를 내리기는 어려우나, 어느 구간을 넘어간다면 flat 구간에 도착할 것이다 $\\to$ 안정적인 구간, loss가 가장 작은 곳을 가지는 구간\n",
    ">\n",
    "> 맞나? $\\to$ 경험적으로 많이 나옴 + 해당 상황을 조작하여 만들어낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b7d91",
   "metadata": {},
   "source": [
    "* Key updating formula\n",
    "\n",
    "$$w_t \\leftarrow w_{t-1} - η \\nabla l_{w_{t-1}}(\\mathcal{B})$$\n",
    "\n",
    "> 미니배치에 대해 그래디언트를 계산한 후, 작은 learning_late로 업데이트하는 과정을 반복 : epoch\n",
    ">\n",
    "> learning_late는 step $t$마다 일정한 규칙에 의해 줄어드는 것이 이상적임 $\\sum_t η_t = \\infty, ~ \\sum_t η_t^2 < \\infty \\to \\text{e.g.,} ~~ η_t = \\frac1t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8337647",
   "metadata": {},
   "source": [
    "* Scheduler\n",
    "\n",
    "> Learning Rate를 적절히 조절\n",
    ">\n",
    "> Cyclic : 중간에 증가시킴 $\\to$ NLP 쪽에서 사용하는 것이 좋다고 알려져 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56392f",
   "metadata": {},
   "source": [
    "`-` Optimizer\n",
    "\n",
    "> Adam을 일반적으로 사용\n",
    "\n",
    "```{raw}\n",
    "Compute stochastic gradient: nabla\n",
    "Update biased ㅇㅅㅇ\n",
    "```\n",
    "\n",
    "> 길어요. 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae2066",
   "metadata": {},
   "source": [
    "`-` Data Loader\n",
    "\n",
    "> 데이터 용량 엄청 큼 $\\to$ 바로는 못올림, 병렬로 GPU 메모리를 늘려도 상호간 통신의 문제로 곤란함\n",
    ">\n",
    "> 대용량의 데이터를 여러 배치로 쪼개서 순차적으로 입력 후 삭제 : GPU 메모리 상한보다 작은 크기의 배치. storage 관점에서는 제약이 없어짐\n",
    ">\n",
    "> 그럼에도 불구하고 GPU 메모리는 계속해서 늘어남 $\\to$ LLM 학습 시 최대 토큰 length와 관련된 문제이기 때문에 GPU 용량이 중요해짐\n",
    ">\n",
    "> Data Loader가 해당 문제를 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada1886",
   "metadata": {},
   "source": [
    "## 3. 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81405a",
   "metadata": {},
   "source": [
    "`-` 모수 두 가지\n",
    "\n",
    "1. X의 변환과 관련된 모수 : non-Convex $(f) \\to$ 깊게 쌓기 때문에 많은 파라미터를 차지 $\\to$ 많은 정보가 포함되어 있을 것이라고 예상 가능함\n",
    "2. 손실함수와 관련된 모수 : Convex $(l)$\n",
    "\n",
    "> non-Convex + Convex $\\to$ non-Convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7fd98",
   "metadata": {},
   "source": [
    "`-` Inovation\n",
    "\n",
    "* 레이어를 Deep하게 쌓음 $\\to$ 레이어를 깊게 쌓을수록 여러가지 장점이 있음\n",
    "* Deep Layer의 학습 과정 : 정보 추출 과정을 합성함수를 사용한 변환으로 해결\n",
    "> 정보 추출의 규칙과 원리가 명확히 설명 가능하지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0a20f",
   "metadata": {},
   "source": [
    "`-` 역전파 backpropaganda\n",
    "\n",
    "* 모든 뉴런(Weight)을 한번에 수정해야 함"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
