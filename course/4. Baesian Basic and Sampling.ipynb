{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf10d55e",
   "metadata": {},
   "source": [
    "# Baesian Basics and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7064c5e",
   "metadata": {},
   "source": [
    "## 1. Bayesian Intro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475c765",
   "metadata": {},
   "source": [
    "### Bayes Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584325f5",
   "metadata": {},
   "source": [
    "$$P(Z|X) = \\frac{P(X|Z)P(Z)}{P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12906bd7",
   "metadata": {},
   "source": [
    "> conditional probability를 inverse-probability로 표현\n",
    "\n",
    "* $Z$: unknown random. 관측 불가능\n",
    "* $X$: $Z$에 의해서 결정되며, 관측 가능\n",
    "\n",
    "> 관측되지 않은 $Z$의 확률를 관측된 사건 $X$로 표현하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8457ca0",
   "metadata": {},
   "source": [
    "`-` $Z$에 확률을 줄 수 있다고 말함: philosophical issue로 판별\n",
    "\n",
    "* $Z$를 unknown quantity (as fixed)로 보면 상당히 다루기 어려우나, 랜덤으로 보면 쉬움\n",
    "* 확률 변수를 동원하여, 그 확률 변수로부터 규칙을 설명하는 것\n",
    "\n",
    "> DNN random initial -> random result. $\\to$ prediction이 고정되지 못함. 베이지안 관점에서 맞는 사실임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8efa64",
   "metadata": {},
   "source": [
    "`-` Inference / prediction\n",
    "\n",
    "* Inference: 기저의 분포를 식별. $\\to$ 샘플로부터 모집단에 대한 여러 정보 및 관계를 알아내는 것. 추정.\n",
    "* Prediction: 예측의 영역. $\\to$ 베이지안에서의 중요 개념. 확률적으로 prediction의 개념을 정교화\n",
    "\n",
    "> hypothesis: 기계학습에서는 모델의 의미로서 사용되는 경향이..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95cd60",
   "metadata": {},
   "source": [
    "`Example`: 0/1의 특정 숫자가 주어졌을 때, 1이 나올 확률 (수식을 이해하는 것까지 바라진 않는다...?)\n",
    "\n",
    "$$R_i \\sim \\text{Bernoulli}(w), ~ i \\in [n], ~ w \\sim \\text{Beta}(1, 1)$$\n",
    "\n",
    "> $w$를 unknown parameter가 아닌 확률변수라고 가정 (Uniform Prior)\n",
    "\n",
    "$$p(r=1|R_1, R_2, ..., R_n)?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20717caa",
   "metadata": {},
   "source": [
    "`Solution`\n",
    "\n",
    "* $w$를 알고 있다면, 그냥 획률은 $w$가 될 것... 관측값에 대하여 $w$는 어떻게 바뀌는가?\n",
    "\n",
    "> MLE에서 비율로 사용하는 것과 동일하지 않다.\n",
    ">\n",
    "> Prior가 바뀌면 확률이 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c985958",
   "metadata": {},
   "source": [
    "$$\\frac{\\sum_i R_i + 1}{n + 2} ~~~~ \\frac{\\sum_i R_i}{n}$$\n",
    "\n",
    "> MLE와는 Prior 때문에 추정량이 다름\n",
    "\n",
    "\n",
    "* $\\hat p$: MLE, $\\hat p_B$: Baysian\n",
    "* 0.5를 중심으로 안쪽으로 모이도록 조정됨 $\\to$ $\\hat p$을 0.5쪽으로 조금 끌어당긴 것이 $\\hat p_B$이다.\n",
    "* 최종적으로는 Prior의 평균 방향으로 당겨진 것 (여기선 Uniform): Shirinkage Effect\n",
    "\n",
    "> Prior를 어떻게 설정하는지에 따라 Shirinkage 방향이 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d26c3",
   "metadata": {},
   "source": [
    "`-` Interpretation\n",
    "\n",
    "* n과 $\\sum R_i$가 충분히 크다면, prior의 영향이 작아짐\n",
    "* 보통은 prior를 어떻게 주어도 MLE보다 약간 값이 작게 나오는 경향이 많음 $\\to$ 성능이 더 좋아지는 경우가 있음\n",
    "> 아무리 값이 작아도, 그 값이 매우 다수인 경우 그 영향력이 클 수 있다. $\\to$ shirinkage를 도입하여 큰 값은 덜 줄이고 작은 값은 많이 줄임. sample mean 쓰는 것 보다 prior를 잘 사용함으로써 여러가지 상황에서의 문제를 줄일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2db20",
   "metadata": {},
   "source": [
    "`-` 정리\n",
    "\n",
    "* 모수를 변화시키면서 예측을 수행시킬 수 있으며, 이는 일반적으로 평균과 다르다. (대수의 법칙에 따라 평균과 유사)\n",
    "* 평균보다 절대값이 작아지는 것이 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc69d64",
   "metadata": {},
   "source": [
    "## 2. Prior, Model, and Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d2e72",
   "metadata": {},
   "source": [
    "### Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7f6c0",
   "metadata": {},
   "source": [
    "* $p(\\theta)$: random parameter에 대한 확률 (pdf) $\\to$ law or **prior** p\n",
    "* $p(y | \\theta)$: data distribution given $\\theta$\n",
    "* $L(\\theta) = p(y | \\theta)$: $y$를 fixed scalar로 취급할 때, $\\theta$의 값을 변화시킴에 따라 변화하는 Likelihood\n",
    "* $p(\\theta | y)$: posterior distribution given $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cf0b8",
   "metadata": {},
   "source": [
    "`-` Posterior and usage of its components.\n",
    "\n",
    "$$p(\\theta | y) \\propto L(\\theta | y)p(\\theta)$$\n",
    "\n",
    "* $p(y)$는 상수 같은 거니까 그대로 둬도 됨\n",
    "* $\\theta$가 argument라는 점을 명시해주기 위해서 Likelihood로 정의를 바꿈\n",
    "* 커널만 알면, 상수로 취급되는 부분은 전부 유도해낼 수 있음 $\\to$ 어차피 적분값으로 나누면, 확률밀도함수의 기본 성질을 만족하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dce81",
   "metadata": {},
   "source": [
    "`-` Prior\n",
    "\n",
    "* Distribution for parameter (when no observation) $\\to$ 정보를 입력. (평균, 분산, shape, ... 등등의 subject)\n",
    "* Construction\n",
    "> 1. 해당 도메인의 전문가들이 모여 분포를 파악 elicitation\n",
    "> 2. 평균, 분산만 대충 파악하고 가우시안/베타 분포...\n",
    "> 3. **non-informative**: prior가 gaussian이면 mean에 몰리므로 데이터가 들어와도 $L(\\theta|y)$가 왜곡될 수 있음. uniform이여도 구간이 정해져 있어서 곤란... 그냥 $p(w) = c, -\\infty < w < \\infty$를 사용. $\\to$ posterior를 들여왔을 때 적분해서 1이 되기만 하면 충분하다...\n",
    ">\n",
    ">   Example: $p(\\mu) ∝ 1, p(\\sigma) ∝ \\frac{1}{\\sigma} \\to$ scale이 바뀌어도 형태가 계속해서 유지됨...\n",
    "\n",
    "* 샘플이 커지면 prior effect가 작아짐 ㅇㅇ\n",
    "* Complex가 높은 모델(DNN: Weight + Bias에 모두 Prior를 줘야 함)의 경우, 샘플이 Prior Effect를 압도할 수 없게 됨\n",
    "> 이때문에 베이지안에서 딥러닝을 사용하기 어려움... non-informative도 적분이 안되는 경우도 있음..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26735992",
   "metadata": {},
   "source": [
    "`-` Model or Likelihood\n",
    "\n",
    "* 데이터는 posterior에 Likelihood로 영향을 줄 수 있음\n",
    "* 데이터의 크기가 커질수록 Posterior에 데이터가 주는 영향력이 커짐 ㅇㅇ\n",
    "\n",
    "`-` Posterior\n",
    "\n",
    "* random parameter의 posterior $\\to$ 분포를 추정하는 문제\n",
    "* Posterior는 분포임 (point estimator가 아님. 분포를 만들어버린 다음에 점과 구간을 뽑아내는 것. 훨씬 informative한 객체)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db11e15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
