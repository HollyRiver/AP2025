{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71463ab7",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d172b",
   "metadata": {},
   "source": [
    "## 1. Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351a21c",
   "metadata": {},
   "source": [
    "* pre-trained model이 주어졌을 때, 전이가 유용한가?\n",
    "\n",
    "> DNN에서 feature extractor + loss에서 feature extractor 역할을 하는 부분은 다른 곳에 활용해도 잘 작동하지 않을까? 라는 생각\n",
    ">\n",
    "> General한 pre-trained classifier를 다른 도메인에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121046d",
   "metadata": {},
   "source": [
    "`-` Deep Belief Network\n",
    "\n",
    "> Loss function이 없는 비지도학습 네트워크 개념\n",
    ">\n",
    "> 얘의 결과로 지도학습을 했더니, 결과가 좋더라 $\\to$ DNN에서의 확장 가능성?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cdd5c",
   "metadata": {},
   "source": [
    "`-` Transfer Learning\n",
    "\n",
    "* $\\hat{f}$에 $x$를 삽입하여 feature를 추출 : Source model\n",
    "* feature extractor에 input을 넣으면 숫자로 이뤄진 노드가 나옴\n",
    "* 추출된 피쳐를 여러 개의 MLP에 입력하여 타겟 분류\n",
    "\n",
    "> 모을 수 있는 데이터가 한정된 상황 + 컴퓨팅 자원도 한정된 상황 $\\to$ 전이 학습으로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f3d78",
   "metadata": {},
   "source": [
    "`-` 핵심 가정\n",
    "\n",
    "> 기존 커다란 소스 모델이 새로운 문제를 푸는 데에 도움이 되어야 함\n",
    "\n",
    "1. $g_i : \\mathcal{X}_s \\to \\mathcal{Y}_s$ hypothesis of source tasks\n",
    "2. $f : \\mathcal{X}_t \\to \\mathcal{Y}_s$ hypothesis of target task\n",
    "\n",
    "$$g_i=w_i \\circ c, ~ f = v \\circ c. ~~ c \\in \\mathcal{C}, v \\in \\mathcal{V}$$\n",
    "\n",
    "> $c$는 소스 데이터에서 학습 가능, $v$는 타겟 데이터로 학습 가능\n",
    ">\n",
    "> $g_i, f$가 공유하는 특징 $c$가 커야 전이 학습이 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb387b3",
   "metadata": {},
   "source": [
    "`-` Freezing, fine-tuning, additional layers\n",
    "\n",
    "> 일단 소스 모델의 마지막 output layer를 제거하고, target task를 위한 additional layers를 부착, end-to-end 구조로 이뤄짐\n",
    "\n",
    "* Scratch : target 데이터로만 퓨어하게 학습\n",
    "* Freezing : additional layers만 훈련. 소스 모델의 가중치는 동결 $\\to$ 새로 붙여놓은 레이어만 학습함\n",
    "* fine-tuning : 소스 모델의 가중치 = 정보가 있는 초기값은 일부만 조정됨 $\\to$ 미세 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f435c",
   "metadata": {},
   "source": [
    "* 전체 모델을 재훈련 : 데이터가 적은 경우 모형이 일그러짐\n",
    "* fine-tuning : feature extractor의 일부 레이어를 동결시키고, 일부 레이어만 같이 훈련\n",
    "> 부착된 레이어는 랜덤 값을 할당하여 훈련, feature extractor의 가중치는 기존 값을 시작점으로 하여 미세하게만 바뀜\n",
    "* additional layers : 부착한 레이어만 훈련, 도메인이 비슷한 경우 유리\n",
    "\n",
    "> additional layers만 훈련하는 경우에서 성능이 좋다면 그대로 사용하는 게 가장 좋음 : 소스 모델의 weight를 바꾸지 않으므로 계산 효율적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7362ee",
   "metadata": {},
   "source": [
    "## 2. Mathmatical frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84bef9",
   "metadata": {},
   "source": [
    "* $C(\\mathcal{C}), C(\\mathcal{V})$를 task $\\mathcal{C}, \\mathcal{V}$의 복잡도 측정값이라 하면 : Common / perfectly new task V\n",
    "\n",
    "$$\\tilde{O} \\left( \\frac{1}{\\nu} \\sqrt{\\frac{C(\\mathcal{C}) + t C(\\mathcal{V})}{nt}} \\right)$$\n",
    "\n",
    "> n은 소스에서의 훈련 데이터셋 크기, m은 타겟에서의 데이터셋 크기, t는 task의 수\n",
    ">\n",
    "> 일단 m을 키우면 복잡도가 줄어들긴 함 $\\to$ target sample size는 크기를 키우기 쉽지 않음...\n",
    ">\n",
    "> 소스와 타겟의 공통 태스크 비중이 커야 복잡도가 줄어듦, nt가 커도 복잡도가 줄어듦(소스 모델이 큰 모델일 수록) $\\to$ 복잡한 소스 모델을 가져오고, 작은 문제를 풀이할 수록 전이학습의 효과가 좋음\n",
    ">\n",
    "> 좋은 소스를 찾아서 타겟 문제를 해결하면 됨 ㅇㅇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3f822",
   "metadata": {},
   "source": [
    "`-` 모델 저장 형태\n",
    "\n",
    "1. model & weight를 저장 : Llama\n",
    "2. weight만 저장 $\\to$ 똑같은 모델을 선언해야만 로드할 수 있음 $\\to$ 이건 좀..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91939f96",
   "metadata": {},
   "source": [
    "## 3. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb133e",
   "metadata": {},
   "source": [
    "`-` Efficientnet\n",
    "\n",
    "> 모델 아키텍쳐를 바꾸지 말고, 여러가지 파라미터들을 조화롭게 구성해보자 : 이미지 사이즈 / 레이어 개수 / 레이어 크기 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d770b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
